{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb8c0ce4",
   "metadata": {},
   "source": [
    "# Palm Case Study - ResortChain Financial Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ab2c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edb23f",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf6a704",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "This code cell loads all required libraries and datasets, configures plotting defaults, and performs initial data validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391babe8",
   "metadata": {
    "tags": [
     "dataset_import"
    ]
   },
   "outputs": [],
   "source": [
    "# CELL 1: SETUP & DATA LOADING\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Plotly defaults\n",
    "px.defaults.template = 'plotly_white'\n",
    "px.defaults.width = 1100\n",
    "px.defaults.height = 500\n",
    "\n",
    "# Configure dataset paths (absolute)\n",
    "base_dir = Path(\"/Users/gianniskotsas/Documents/Side Projects/palm-case-study/scripts/datasets/raw\")\n",
    "transactions_path = base_dir / \"transactions.csv\"\n",
    "balances_path = base_dir / \"balances.csv\"\n",
    "system_forecasts_path = base_dir / \"system_forecasts.csv\"\n",
    "user_forecasts_path = base_dir / \"user_forecasts.csv\"\n",
    "\n",
    "# Helper: choose main date column name from available candidates\n",
    "# IMPORTANT: value_date is prioritized over booking_date\n",
    "DATE_CANDIDATES = [\n",
    "    'value_date', 'booking_date', 'date', 'transaction_date', 'transactionDate'\n",
    "]\n",
    "\n",
    "def pick_date_column(df: pd.DataFrame) -> str | None:\n",
    "    for c in DATE_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # try to detect first column with datetime dtype\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
    "            return c\n",
    "    # try to parse any column ending with 'date'\n",
    "    for c in df.columns:\n",
    "        if 'date' in c.lower():\n",
    "            try:\n",
    "                _ = pd.to_datetime(df[c], errors='raise')\n",
    "                return c\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "# Load Transactions: support CSV (expected) or Excel fallback\n",
    "transactions = None\n",
    "if transactions_path.suffix.lower() in {'.xlsx', '.xls'}:\n",
    "    # Excel with potential multiple sheets\n",
    "    xl = pd.ExcelFile(transactions_path)\n",
    "    sheet_names = set([s.lower() for s in xl.sheet_names])\n",
    "    try:\n",
    "        tx_sheet = next(s for s in xl.sheet_names if s.lower() in {\"transactions\", \"tx\", \"data\"})\n",
    "    except StopIteration:\n",
    "        tx_sheet = xl.sheet_names[0]\n",
    "    transactions = pd.read_excel(xl, sheet_name=tx_sheet)\n",
    "else:\n",
    "    # CSV (semicolon sep, comma decimal)\n",
    "    parse_cols = ['value_date', 'booking_date']\n",
    "    parse_present = [c for c in parse_cols if c in pd.read_csv(transactions_path, sep=';', nrows=0).columns]\n",
    "    transactions = pd.read_csv(\n",
    "        transactions_path,\n",
    "        sep=';',\n",
    "        decimal=',',\n",
    "        dtype={\n",
    "            'account_number': 'string',\n",
    "            'currency': 'string',\n",
    "            'credit_or_debit': 'string',\n",
    "            'additional_info': 'string',\n",
    "            'remittence_info': 'string',\n",
    "            'bank_reference': 'string',\n",
    "            'creditor_name': 'string',\n",
    "            'debtor_name': 'string',\n",
    "        },\n",
    "        parse_dates=parse_present if len(parse_present) > 0 else None,\n",
    "        dayfirst=True,\n",
    "        infer_datetime_format=True,\n",
    "        engine='python'\n",
    "    )\n",
    "\n",
    "# Load optional datasets when present\n",
    "balances = None\n",
    "if balances_path.exists():\n",
    "    try:\n",
    "        balances = pd.read_csv(balances_path, sep=';', decimal=',', parse_dates=['balance_date'])\n",
    "    except Exception:\n",
    "        balances = pd.read_csv(balances_path, sep=';', decimal=',')\n",
    "        if 'balance_date' in balances.columns:\n",
    "            balances['balance_date'] = pd.to_datetime(balances['balance_date'], errors='coerce')\n",
    "\n",
    "system_forecasts = None\n",
    "if system_forecasts_path.exists():\n",
    "    system_forecasts = pd.read_csv(system_forecasts_path, sep=';', decimal=',')\n",
    "\n",
    "user_forecasts = None\n",
    "if user_forecasts_path.exists():\n",
    "    user_forecasts = pd.read_csv(user_forecasts_path, sep=';', decimal=',')\n",
    "\n",
    "# Identify main date column for transactions\n",
    "trx_date_col = pick_date_column(transactions) if transactions is not None else None\n",
    "if trx_date_col is not None:\n",
    "    transactions[trx_date_col] = pd.to_datetime(transactions[trx_date_col], errors='coerce')\n",
    "\n",
    "# Basic summary\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Transactions loaded: {transactions is not None}\")\n",
    "if transactions is not None:\n",
    "    print(f\"  Shape: {transactions.shape}\")\n",
    "    print(f\"  ðŸ“… DATE COLUMN USED: {trx_date_col}\")\n",
    "print(f\"\\nBalances loaded: {balances is not None}\")\n",
    "if balances is not None:\n",
    "    print(f\"  Shape: {balances.shape}\")\n",
    "print(f\"\\nSystem forecasts loaded: {system_forecasts is not None}\")\n",
    "if system_forecasts is not None:\n",
    "    print(f\"  Shape: {system_forecasts.shape}\")\n",
    "print(f\"\\nUser forecasts loaded: {user_forecasts is not None}\")\n",
    "if user_forecasts is not None:\n",
    "    print(f\"  Shape: {user_forecasts.shape}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Peek\n",
    "display(transactions.head(5) if transactions is not None else \"transactions missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dda0e1",
   "metadata": {},
   "source": [
    "## Monthly Expenses vs Revenue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5862d14",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Generates the monthly expenses chart showing stacked bars for expense categories (Payroll, Tax, FX, Investment, Insurance, Maintenance) with hotel revenue as an overlaid line. Exports data in TypeScript-ready format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f07af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART 1: Monthly Expenses by Category (Stacked Bar) + Hotel Revenue (Line)\n",
    "\n",
    "# Prepare data\n",
    "tx = transactions.copy()\n",
    "tx['month'] = pd.to_datetime(tx[trx_date_col]).dt.to_period('M')\n",
    "\n",
    "# Extract category from additional_info\n",
    "tx['category'] = tx['additional_info'].str.extract(r'(.*?) - ')[0]\n",
    "\n",
    "# Separate expenses (DBIT) and revenue (CRDT)\n",
    "expenses = tx[(tx['credit_or_debit'] == 'DBIT') & (tx['category'].notna())]\n",
    "revenue = tx[(tx['credit_or_debit'] == 'CRDT')]\n",
    "\n",
    "# Aggregate monthly expenses by category\n",
    "monthly_expenses = expenses.groupby(['month', 'category'])['amount'].sum().reset_index()\n",
    "monthly_expenses['month_str'] = monthly_expenses['month'].astype(str)\n",
    "\n",
    "# Aggregate monthly revenue\n",
    "monthly_revenue = revenue.groupby('month')['amount'].sum().reset_index()\n",
    "monthly_revenue['month_str'] = monthly_revenue['month'].astype(str)\n",
    "\n",
    "# Create stacked bar chart for expenses\n",
    "fig1 = go.Figure()\n",
    "\n",
    "# Add expense bars by category\n",
    "categories = monthly_expenses['category'].unique()\n",
    "for category in sorted(categories):\n",
    "    cat_data = monthly_expenses[monthly_expenses['category'] == category]\n",
    "    fig1.add_trace(go.Bar(\n",
    "        x=cat_data['month_str'],\n",
    "        y=cat_data['amount'],\n",
    "        name=category,\n",
    "        hovertemplate='%{x}<br>%{fullData.name}: â‚¬%{y:,.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Add revenue line\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=monthly_revenue['month_str'],\n",
    "    y=monthly_revenue['amount'],\n",
    "    name='Hotel Revenue',\n",
    "    mode='lines+markers',\n",
    "    line=dict(color='darkgreen', width=3),\n",
    "    marker=dict(size=8),\n",
    "    yaxis='y2',\n",
    "    hovertemplate='%{x}<br>Revenue: â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig1.update_layout(\n",
    "    title='Monthly Expenses by Category & Hotel Revenue',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='Expenses (EUR)', side='left'),\n",
    "    yaxis2=dict(title='Revenue (EUR)', overlaying='y', side='right'),\n",
    "    barmode='stack',\n",
    "    legend=dict(orientation='v', yanchor='top', y=1, xanchor='left', x=1.15),\n",
    "    hovermode='x unified',\n",
    "    height=600,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "# Calculate and display average percentage share of each expense category\n",
    "print(\"\\n=== Average Percentage Share of Expense Categories ===\")\n",
    "total_expenses_by_category = expenses.groupby('category')['amount'].sum().reset_index()\n",
    "total_expenses_overall = total_expenses_by_category['amount'].sum()\n",
    "total_expenses_by_category['percentage'] = (total_expenses_by_category['amount'] / total_expenses_overall) * 100\n",
    "total_expenses_by_category = total_expenses_by_category.sort_values('percentage', ascending=False)\n",
    "\n",
    "for idx, row in total_expenses_by_category.iterrows():\n",
    "    print(f\"{row['category']}: {row['percentage']:.2f}% (â‚¬{row['amount']:,.2f})\")\n",
    "print(f\"\\nTotal Expenses: â‚¬{total_expenses_overall:,.2f}\")\n",
    "\n",
    "# Create DataFrame for CSV export\n",
    "# Pivot expenses to have categories as columns\n",
    "monthly_expenses_pivot = monthly_expenses.pivot(index='month', columns='category', values='amount').reset_index()\n",
    "\n",
    "# Rename columns to match expected format\n",
    "category_mapping = {\n",
    "    'Payroll Payment': 'Payroll',\n",
    "    'Tax Payment': 'Tax',\n",
    "    'FX Transaction': 'FX',\n",
    "    'Investment Purchase': 'Investment',\n",
    "    'Insurance Premium': 'Insurance',\n",
    "    'Maintenance Expenses': 'Maintenance'\n",
    "}\n",
    "monthly_expenses_pivot = monthly_expenses_pivot.rename(columns=category_mapping)\n",
    "\n",
    "# Add revenue column\n",
    "monthly_revenue_for_merge = monthly_revenue.copy()\n",
    "monthly_revenue_for_merge['month'] = monthly_revenue_for_merge['month'].astype(str).apply(lambda x: pd.Period(x, freq='M'))\n",
    "monthly_expenses_pivot = monthly_expenses_pivot.merge(\n",
    "    monthly_revenue_for_merge[['month', 'amount']], \n",
    "    on='month', \n",
    "    how='left'\n",
    ").rename(columns={'amount': 'Revenue'})\n",
    "\n",
    "# Convert month to date string\n",
    "monthly_expenses_pivot['date'] = monthly_expenses_pivot['month'].astype(str)\n",
    "monthly_expenses_pivot = monthly_expenses_pivot.drop('month', axis=1)\n",
    "\n",
    "# Fill NaN with 0 and reorder columns\n",
    "expected_columns = ['date', 'Maintenance', 'Insurance', 'Investment', 'FX', 'Tax', 'Payroll', 'Revenue']\n",
    "for col in expected_columns:\n",
    "    if col not in monthly_expenses_pivot.columns and col != 'date':\n",
    "        monthly_expenses_pivot[col] = 0\n",
    "\n",
    "monthly_expenses_pivot = monthly_expenses_pivot[expected_columns].fillna(0)\n",
    "\n",
    "# Create alias for export\n",
    "revenue_expenses_chart = monthly_expenses_pivot.copy()\n",
    "\n",
    "print(\"\\nâœ“ DataFrame 'revenue_expenses_chart' created for export\")\n",
    "print(f\"Shape: {revenue_expenses_chart.shape}\")\n",
    "display(revenue_expenses_chart)\n",
    "\n",
    "# Format output for JavaScript/TypeScript\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPY-PASTE READY FORMAT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"const data = [\")\n",
    "\n",
    "for idx, row in revenue_expenses_chart.iterrows():\n",
    "    # Convert date string \"2025-01\" to \"Jan 25\" format\n",
    "    month_period = pd.Period(row['date'], freq='M')\n",
    "    date_formatted = month_period.strftime('%b %y')\n",
    "    \n",
    "    print(f\"  {{ date: \\\"{date_formatted}\\\", Maintenance: {row['Maintenance']}, Insurance: {row['Insurance']}, Investment: {row['Investment']}, FX: {row['FX']}, Tax: {row['Tax']}, Payroll: {row['Payroll']}, Revenue: {row['Revenue']} }},\")\n",
    "\n",
    "print(\"];\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505822ac",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Exports the revenue and expenses data to CSV format for potential external use or archival.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Export the revenue and expenses chart data to CSV using the DataFrame `revenue_expenses_chart`\n",
    "# (If your DataFrame is named differently, update the variable name below accordingly.)\n",
    "\n",
    "# Check if the DataFrame exists in the environment\n",
    "if \"revenue_expenses_chart\" in locals():\n",
    "    export_df = revenue_expenses_chart.copy()\n",
    "elif \"monthly_expenses_pivot\" in locals():\n",
    "    # Fallback: try to use monthly_expenses_pivot if available\n",
    "    export_df = monthly_expenses_pivot.copy()\n",
    "else:\n",
    "    raise NameError(\"No DataFrame named 'revenue_expenses_chart' or 'monthly_expenses_pivot' found. Please define it before exporting.\")\n",
    "\n",
    "# Ensure the columns exist and are in the correct order\n",
    "csv_columns = [\"date\", \"Maintenance\", \"Insurance\", \"Investment\", \"FX\", \"Tax\", \"Payroll\", \"Revenue\"]\n",
    "missing_cols = [col for col in csv_columns if col not in export_df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in export DataFrame: {missing_cols}\")\n",
    "\n",
    "# Reorder columns and convert to list of dicts\n",
    "export_data = export_df[csv_columns].to_dict(orient=\"records\")\n",
    "\n",
    "csv_filename = \"revenue_expenses_chart_data.csv\"\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    for row in export_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Chart data exported to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715db123",
   "metadata": {},
   "source": [
    "## Cash Flow Drivers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4cd91",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Calculates monthly averages, percentage shares, and trend data for all expense and revenue categories. Outputs TypeScript-formatted data for dashboard spark charts showing each category's contribution and monthly pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefa9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASH FLOW DRIVERS: Calculate monthly values, averages, and percentage shares\n",
    "\n",
    "# Use the existing data from previous cells\n",
    "# Expenses by category (monthly)\n",
    "expense_categories = ['Maintenance', 'Insurance', 'Investment', 'FX', 'Tax', 'Payroll']\n",
    "revenue_categories = ['Resort Revenue', 'Investment Income', 'Tax Refund']\n",
    "\n",
    "# Get monthly expense data (already computed in earlier cells)\n",
    "# From revenue_expenses_chart DataFrame\n",
    "expenses_df = revenue_expenses_chart[['date'] + expense_categories].copy()\n",
    "revenue_income_df = monthly_income_pivot[['date'] + revenue_categories].copy()\n",
    "\n",
    "# Calculate totals\n",
    "total_expenses_per_month = expenses_df[expense_categories].sum(axis=1)\n",
    "total_revenue_per_month = revenue_income_df[revenue_categories].sum(axis=1)\n",
    "\n",
    "# Calculate overall totals and averages\n",
    "total_expenses_all = expenses_df[expense_categories].sum().sum()\n",
    "total_revenue_all = revenue_income_df[revenue_categories].sum().sum()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CASH FLOW DRIVERS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build the cash flow drivers data structure\n",
    "cash_flow_drivers = {\n",
    "    'expenses': {},\n",
    "    'revenue': {}\n",
    "}\n",
    "\n",
    "print(\"\\n--- EXPENSE CATEGORIES ---\\n\")\n",
    "for category in expense_categories:\n",
    "    monthly_values = expenses_df[category].tolist()\n",
    "    avg_monthly = expenses_df[category].mean()\n",
    "    total_category = expenses_df[category].sum()\n",
    "    share_pct = (total_category / total_expenses_all) * 100\n",
    "    \n",
    "    cash_flow_drivers['expenses'][category] = {\n",
    "        'avg': round(avg_monthly, 2),\n",
    "        'share': round(share_pct, 2),\n",
    "        'data': [round(v, 2) for v in monthly_values]\n",
    "    }\n",
    "    \n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  Average: â‚¬{avg_monthly:,.2f}/month\")\n",
    "    print(f\"  Share: {share_pct:.2f}% of total expenses\")\n",
    "    print(f\"  Monthly values: {[round(v, 2) for v in monthly_values]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n--- REVENUE CATEGORIES ---\\n\")\n",
    "for category in revenue_categories:\n",
    "    monthly_values = revenue_income_df[category].tolist()\n",
    "    avg_monthly = revenue_income_df[category].mean()\n",
    "    total_category = revenue_income_df[category].sum()\n",
    "    share_pct = (total_category / total_revenue_all) * 100\n",
    "    \n",
    "    cash_flow_drivers['revenue'][category] = {\n",
    "        'avg': round(avg_monthly, 2),\n",
    "        'share': round(share_pct, 2),\n",
    "        'data': [round(v, 2) for v in monthly_values]\n",
    "    }\n",
    "    \n",
    "    print(f\"{category}:\")\n",
    "    print(f\"  Average: â‚¬{avg_monthly:,.2f}/month\")\n",
    "    print(f\"  Share: {share_pct:.2f}% of total revenue\")\n",
    "    print(f\"  Monthly values: {[round(v, 2) for v in monthly_values]}\")\n",
    "    print()\n",
    "\n",
    "# Generate TypeScript-ready format\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPY-PASTE READY FORMAT FOR TYPESCRIPT:\")\n",
    "print(\"=\"*80)\n",
    "print(\"const cashFlowDrivers = {\")\n",
    "print(\"  expenses: {\")\n",
    "\n",
    "# Sort expenses by share (largest to smallest)\n",
    "sorted_expenses = sorted(\n",
    "    cash_flow_drivers['expenses'].items(), \n",
    "    key=lambda x: x[1]['share'], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, (category, data) in enumerate(sorted_expenses):\n",
    "    comma = \",\" if i < len(sorted_expenses) - 1 else \"\"\n",
    "    data_str = \", \".join(str(v) for v in data['data'])\n",
    "    print(f\"    '{category}': {{ avg: {data['avg']}, share: {data['share']}, data: [{data_str}] }}{comma}\")\n",
    "\n",
    "print(\"  },\")\n",
    "print(\"  revenue: {\")\n",
    "\n",
    "# Sort revenue by share (largest to smallest)\n",
    "sorted_revenue = sorted(\n",
    "    cash_flow_drivers['revenue'].items(), \n",
    "    key=lambda x: x[1]['share'], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, (category, data) in enumerate(sorted_revenue):\n",
    "    comma = \",\" if i < len(sorted_revenue) - 1 else \"\"\n",
    "    data_str = \", \".join(str(v) for v in data['data'])\n",
    "    print(f\"    '{category}': {{ avg: {data['avg']}, share: {data['share']}, data: [{data_str}] }}{comma}\")\n",
    "\n",
    "print(\"  }\")\n",
    "print(\"};\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nTotal Expenses: â‚¬{total_expenses_all:,.2f}\")\n",
    "print(f\"Total Revenue: â‚¬{total_revenue_all:,.2f}\")\n",
    "print(f\"Number of months: {len(expenses_df)}\")\n",
    "print(f\"\\nâœ“ Cash flow drivers data ready for dashboard spark charts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fc8f1",
   "metadata": {},
   "source": [
    "## Investment Expenses & FX Fees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3acde3",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates grouped bar chart comparing monthly investment expenses and FX fees side-by-side, enabling analysis of these operational costs over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART 6: Exchange Fees Ratio (Line) + Investment Expenses (Bar)\n",
    "\n",
    "# Prepare monthly data - use loaded transactions\n",
    "tx_monthly = transactions.copy()\n",
    "tx_monthly['date'] = pd.to_datetime(tx_monthly[trx_date_col])\n",
    "tx_monthly['month'] = tx_monthly['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate monthly income (CRDT)\n",
    "monthly_income = tx_monthly[tx_monthly['credit_or_debit'] == 'CRDT'].groupby('month')['amount'].sum().reset_index()\n",
    "monthly_income.columns = ['month', 'income']\n",
    "\n",
    "# Calculate monthly exchange fees (FX fees where debtor_name is 'FX Bank')\n",
    "fx_expenses = tx_monthly[\n",
    "    (tx_monthly['credit_or_debit'] == 'DBIT') & \n",
    "    (tx_monthly['debtor_name'] == 'FX Bank')\n",
    "].groupby('month')['amount'].sum().reset_index()\n",
    "fx_expenses.columns = ['month', 'fx_fees']\n",
    "\n",
    "print(f\"FX fees transactions found: {len(tx_monthly[(tx_monthly['credit_or_debit'] == 'DBIT') & (tx_monthly['debtor_name'] == 'FX Bank')])}\")\n",
    "\n",
    "# Calculate monthly investment expenses\n",
    "investment_expenses = tx_monthly[\n",
    "    (tx_monthly['credit_or_debit'] == 'DBIT') & \n",
    "    (tx_monthly['additional_info'].str.contains('Investment', na=False))\n",
    "].groupby('month')['amount'].sum().reset_index()\n",
    "investment_expenses.columns = ['month', 'investment_expenses']\n",
    "\n",
    "# Merge data\n",
    "chart6_data = monthly_income.merge(fx_expenses, on='month', how='left')\n",
    "chart6_data = chart6_data.merge(investment_expenses, on='month', how='left')\n",
    "chart6_data['fx_fees'] = chart6_data['fx_fees'].fillna(0)\n",
    "chart6_data['investment_expenses'] = chart6_data['investment_expenses'].fillna(0)\n",
    "\n",
    "chart6_data['month_str'] = chart6_data['month'].astype(str)\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig6 = go.Figure()\n",
    "\n",
    "# Add investment expenses bars\n",
    "fig6.add_trace(go.Bar(\n",
    "    x=chart6_data['month_str'],\n",
    "    y=chart6_data['investment_expenses'],\n",
    "    name='Investment Expenses',\n",
    "    marker_color='lightcoral',\n",
    "    hovertemplate='%{x}<br>Investment: â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add FX fees bars\n",
    "fig6.add_trace(go.Bar(\n",
    "    x=chart6_data['month_str'],\n",
    "    y=chart6_data['fx_fees'],\n",
    "    name='FX Fees',\n",
    "    marker_color='steelblue',\n",
    "    hovertemplate='%{x}<br>FX Fees: â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout for grouped bars\n",
    "fig6.update_layout(\n",
    "    title='Investment Expenses & FX Fees by Month',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='Amount (EUR)'),\n",
    "    barmode='group',\n",
    "    hovermode='x unified',\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1)\n",
    ")\n",
    "\n",
    "fig6.show()\n",
    "\n",
    "# Display summary stats\n",
    "print(\"\\nMonthly Investment & FX Fees Summary:\")\n",
    "print(chart6_data[['month_str', 'investment_expenses', 'fx_fees']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a0ae0",
   "metadata": {},
   "source": [
    "## Revenue Patterns Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319c7ee",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Generates a heatmap visualization showing resort revenue patterns by day of week and week of year. Reveals business-focused clientele with mid-week peaks and seasonal trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b573578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART 2: Revenue Heatmap (Day of Week vs Week of Year)\n",
    "\n",
    "# Confirm which date column is being used\n",
    "print(f\"Using date column for heatmap: {trx_date_col}\")\n",
    "\n",
    "# Prepare revenue data - use loaded transactions\n",
    "revenue_data = transactions[(transactions['credit_or_debit'] == 'CRDT') & \n",
    "                             (transactions['additional_info'].str.contains('Resort Revenue', na=False))].copy()\n",
    "\n",
    "revenue_data['date'] = pd.to_datetime(revenue_data[trx_date_col])\n",
    "revenue_data['day_of_week'] = revenue_data['date'].dt.day_name()\n",
    "# Convert week to integer properly\n",
    "revenue_data['week_of_year'] = revenue_data['date'].dt.isocalendar().week.astype(int)\n",
    "\n",
    "# Aggregate revenue by week and day\n",
    "heatmap_data = revenue_data.groupby(['week_of_year', 'day_of_week'], as_index=False)['amount'].sum()\n",
    "\n",
    "# Debug: check data\n",
    "print(f\"Heatmap data shape: {heatmap_data.shape}\")\n",
    "print(f\"Sample data:\\n{heatmap_data.head(10)}\")\n",
    "print(f\"Week range: {heatmap_data['week_of_year'].min()} to {heatmap_data['week_of_year'].max()}\")\n",
    "\n",
    "# Pivot for heatmap\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "heatmap_pivot = heatmap_data.pivot(index='day_of_week', columns='week_of_year', values='amount')\n",
    "heatmap_pivot = heatmap_pivot.reindex(day_order)\n",
    "\n",
    "# Replace NaN with 0 for visualization\n",
    "heatmap_pivot = heatmap_pivot.fillna(0)\n",
    "\n",
    "# Create heatmap\n",
    "fig2 = go.Figure(data=go.Heatmap(\n",
    "    z=heatmap_pivot.values,\n",
    "    x=heatmap_pivot.columns.tolist(),\n",
    "    y=heatmap_pivot.index.tolist(),\n",
    "    colorscale='YlOrRd',\n",
    "    hovertemplate='Week %{x}<br>%{y}<br>Revenue: â‚¬%{z:,.2f}<extra></extra>',\n",
    "    colorbar=dict(title='Revenue (EUR)'),\n",
    "    zmid=None  # Auto-scale colors\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='Hotel Revenue Heatmap: Day of Week vs Week of Year',\n",
    "    xaxis=dict(title='Week of Year', type='category'),\n",
    "    yaxis=dict(title='Day of Week'),\n",
    "    height=500,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce637966",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Exports the heatmap data in TypeScript-ready format with complete week coverage (including zero values for days without revenue), suitable for frontend visualization components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export heatmap data in copy-paste ready format for TypeScript/JavaScript\n",
    "\n",
    "# Use the heatmap_data DataFrame from the previous cell\n",
    "# Group by day and create the nested structure\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "# First, let's analyze the data coverage\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA COVERAGE ANALYSIS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get the range of weeks in the data\n",
    "all_weeks = sorted(heatmap_data['week_of_year'].unique())\n",
    "min_week = heatmap_data['week_of_year'].min()\n",
    "max_week = heatmap_data['week_of_year'].max()\n",
    "print(f\"Week range in data: {min_week} to {max_week}\")\n",
    "print(f\"Total weeks with ANY data: {len(all_weeks)}\")\n",
    "\n",
    "# Create a complete grid of all possible day/week combinations\n",
    "all_possible_weeks = range(min_week, max_week + 1)\n",
    "total_possible_combinations = len(day_order) * len(all_possible_weeks)\n",
    "actual_data_points = len(heatmap_data)\n",
    "\n",
    "print(f\"\\nTotal possible day/week combinations: {total_possible_combinations}\")\n",
    "print(f\"Actual data points with revenue > 0: {actual_data_points}\")\n",
    "print(f\"Missing combinations: {total_possible_combinations - actual_data_points}\")\n",
    "print(f\"Coverage: {(actual_data_points / total_possible_combinations * 100):.1f}%\")\n",
    "\n",
    "# Check each day of week\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BREAKDOWN BY DAY OF WEEK:\")\n",
    "print(\"-\"*80)\n",
    "for day in day_order:\n",
    "    day_data = heatmap_data[heatmap_data['day_of_week'] == day]\n",
    "    weeks_with_data = set(day_data['week_of_year'].values)\n",
    "    missing_weeks = set(all_possible_weeks) - weeks_with_data\n",
    "    \n",
    "    print(f\"\\n{day}:\")\n",
    "    print(f\"  Weeks with data: {len(weeks_with_data)} out of {len(all_possible_weeks)}\")\n",
    "    print(f\"  Missing weeks: {len(missing_weeks)}\")\n",
    "    if len(missing_weeks) > 0 and len(missing_weeks) <= 10:\n",
    "        print(f\"  Missing week numbers: {sorted(missing_weeks)}\")\n",
    "    elif len(missing_weeks) > 10:\n",
    "        print(f\"  Missing week numbers: {sorted(list(missing_weeks)[:10])}... (showing first 10)\")\n",
    "\n",
    "# Now let's check the raw revenue data to see if missing means no transactions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHECKING RAW TRANSACTION DATA:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Go back to the original revenue_data to see all days\n",
    "revenue_check = revenue_data.copy()\n",
    "revenue_check['has_revenue'] = revenue_check['amount'] > 0\n",
    "\n",
    "# Count by day of week\n",
    "print(\"\\nTransactions by day of week:\")\n",
    "for day in day_order:\n",
    "    day_count = len(revenue_check[revenue_check['day_of_week'] == day])\n",
    "    day_revenue = revenue_check[revenue_check['day_of_week'] == day]['amount'].sum()\n",
    "    print(f\"  {day}: {day_count} transactions, â‚¬{day_revenue:,.2f} total\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPY-PASTE READY HEATMAP DATA (WITH COMPLETE WEEK COVERAGE):\")\n",
    "print(\"=\"*80)\n",
    "print(\"const data = [\")\n",
    "\n",
    "for day in day_order:\n",
    "    # Filter data for this day\n",
    "    day_data = heatmap_data[heatmap_data['day_of_week'] == day].copy()\n",
    "    \n",
    "    # Create a dictionary for quick lookup of week -> revenue\n",
    "    week_revenue_map = dict(zip(day_data['week_of_year'], day_data['amount']))\n",
    "    \n",
    "    # Format the data array - INCLUDING ALL WEEKS (missing ones get value: 0)\n",
    "    data_entries = []\n",
    "    for week in all_possible_weeks:\n",
    "        # Check if this week has data, otherwise use 0\n",
    "        value = round(week_revenue_map.get(week, 0), 2)\n",
    "        data_entries.append(f'{{ weekOfYear: {week}, index: 1, value: {value} }}')\n",
    "    \n",
    "    # Join all entries for this day\n",
    "    data_str = ', '.join(data_entries)\n",
    "    \n",
    "    print(f'  {{')\n",
    "    print(f'    \"name\": \"{day}\",')\n",
    "    print(f'    \"data\": [{data_str}]')\n",
    "    print(f'  }},')\n",
    "\n",
    "print(\"];\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNote: Complete dataset includes ALL {len(all_possible_weeks)} weeks for each day.\")\n",
    "print(f\"Missing week/day combinations now included with value: 0\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Total data points: {len(heatmap_data)}\")\n",
    "print(f\"- Days with data: {heatmap_data['day_of_week'].nunique()}\")\n",
    "print(f\"- Weeks with data: {heatmap_data['week_of_year'].nunique()}\")\n",
    "print(f\"- Total revenue: â‚¬{heatmap_data['amount'].sum():,.2f}\")\n",
    "print(f\"- Average daily revenue: â‚¬{heatmap_data['amount'].mean():,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d30ed",
   "metadata": {},
   "source": [
    "## Daily Revenue with Holiday Correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef32e5",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates a bar chart of daily income with vertical lines marking French bank holidays. Helps identify correlations between holidays, major events (Monaco GP, Easter) and revenue spikes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae439f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART 5: Daily Income Transactions with French Holidays\n",
    "\n",
    "# Load bank holidays and filter for France\n",
    "holidays_path = base_dir.parent / \"extra_datasets\" / \"bank_holidays_2025.csv\"\n",
    "holidays_df = pd.read_csv(holidays_path)\n",
    "france_holidays = holidays_df[holidays_df['Country'] == 'France'].copy()\n",
    "france_holidays['Date'] = pd.to_datetime(france_holidays['Date'])\n",
    "\n",
    "# Get daily income (CRDT transactions) - use loaded transactions\n",
    "income_data = transactions[transactions['credit_or_debit'] == 'CRDT'].copy()\n",
    "income_data['date'] = pd.to_datetime(income_data[trx_date_col])\n",
    "\n",
    "# Aggregate daily income\n",
    "daily_income = income_data.groupby('date')['amount'].sum().reset_index()\n",
    "daily_income.columns = ['date', 'income']\n",
    "\n",
    "# Create chart\n",
    "fig5 = go.Figure()\n",
    "\n",
    "# Add income bars\n",
    "fig5.add_trace(go.Bar(\n",
    "    x=daily_income['date'],\n",
    "    y=daily_income['income'],\n",
    "    name='Daily Income',\n",
    "    marker_color='lightgreen',\n",
    "    hovertemplate='%{x|%Y-%m-%d}<br>Income: â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add vertical lines for French holidays using shapes\n",
    "for idx, holiday in france_holidays.iterrows():\n",
    "    # Convert Timestamp to datetime for plotly\n",
    "    holiday_date = pd.Timestamp(holiday['Date']).to_pydatetime()\n",
    "    \n",
    "    fig5.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=holiday_date,\n",
    "        x1=holiday_date,\n",
    "        y0=0,\n",
    "        y1=1,\n",
    "        yref=\"paper\",\n",
    "        line=dict(color=\"red\", width=2, dash=\"dash\")\n",
    "    )\n",
    "    \n",
    "    # Add annotation\n",
    "    fig5.add_annotation(\n",
    "        x=holiday_date,\n",
    "        y=1,\n",
    "        yref=\"paper\",\n",
    "        text=holiday['Holiday'],\n",
    "        showarrow=False,\n",
    "        textangle=-90,\n",
    "        font=dict(size=8, color=\"red\"),\n",
    "        yanchor=\"bottom\"\n",
    "    )\n",
    "\n",
    "fig5.update_layout(\n",
    "    title='Daily Income Transactions with French Bank Holidays (2025)',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Income (EUR)'),\n",
    "    hovermode='x unified',\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig5.show()\n",
    "\n",
    "# Create alias for export\n",
    "daily_income_chart = daily_income.copy()\n",
    "french_holidays_chart = france_holidays.copy()\n",
    "\n",
    "print(\"\\nâœ“ DataFrame 'daily_income_chart' created for export\")\n",
    "print(f\"Shape: {daily_income_chart.shape}\")\n",
    "print(f\"Date range: {daily_income_chart['date'].min()} to {daily_income_chart['date'].max()}\")\n",
    "print(f\"Total income: â‚¬{daily_income_chart['income'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\nâœ“ DataFrame 'french_holidays_chart' created for export\")\n",
    "print(f\"French holidays count: {len(french_holidays_chart)}\")\n",
    "\n",
    "# Format output for JavaScript/TypeScript\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPY-PASTE READY FORMAT (DAILY INCOME):\")\n",
    "print(\"=\"*80)\n",
    "print(\"const dailyIncomeData = [\")\n",
    "\n",
    "for idx, row in daily_income_chart.iterrows():\n",
    "    date_formatted = row['date'].strftime('%b %d')\n",
    "    income_value = round(row['income'], 2)\n",
    "    print(f\"  {{ date: '{date_formatted}', income: {income_value} }},\")\n",
    "\n",
    "print(\"];\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPY-PASTE READY FORMAT (FRENCH HOLIDAYS):\")\n",
    "print(\"=\"*80)\n",
    "print(\"const frenchHolidays = [\")\n",
    "\n",
    "for idx, row in french_holidays_chart.iterrows():\n",
    "    date_formatted = row['Date'].strftime('%b %d')\n",
    "    holiday_name = row['Holiday']\n",
    "    print(f\"  {{ date: '{date_formatted}', holiday: '{holiday_name}' }},\")\n",
    "\n",
    "print(\"];\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0bdcc",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Exports monthly income data broken down by category (Resort Revenue, Investment Income, Tax Refund) in TypeScript format for dashboard integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3fb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Monthly Income Data (similar to the example format with categories)\n",
    "\n",
    "# Create monthly aggregation of income\n",
    "monthly_income_breakdown = income_data.copy()\n",
    "monthly_income_breakdown['month'] = monthly_income_breakdown['date'].dt.to_period('M')\n",
    "\n",
    "# Extract category/type from additional_info if available\n",
    "monthly_income_breakdown['category'] = monthly_income_breakdown['additional_info'].str.extract(r'(.*?) - ')[0]\n",
    "\n",
    "# If no clear categories, check for patterns in creditor_name or remittence_info\n",
    "if monthly_income_breakdown['category'].isna().all():\n",
    "    # Try to extract from creditor_name\n",
    "    monthly_income_breakdown['category'] = 'Revenue'  # Default category\n",
    "else:\n",
    "    monthly_income_breakdown['category'] = monthly_income_breakdown['category'].fillna('Other')\n",
    "\n",
    "# Group by month and category\n",
    "monthly_income_by_category = monthly_income_breakdown.groupby(['month', 'category'])['amount'].sum().reset_index()\n",
    "\n",
    "# Pivot to get categories as columns\n",
    "monthly_income_pivot = monthly_income_by_category.pivot(index='month', columns='category', values='amount').reset_index()\n",
    "monthly_income_pivot = monthly_income_pivot.fillna(0)\n",
    "\n",
    "# Convert month to string\n",
    "monthly_income_pivot['date'] = monthly_income_pivot['month'].astype(str)\n",
    "monthly_income_pivot = monthly_income_pivot.drop('month', axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = ['date'] + [col for col in monthly_income_pivot.columns if col != 'date']\n",
    "monthly_income_pivot = monthly_income_pivot[column_order]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MONTHLY INCOME BY CATEGORY:\")\n",
    "print(\"=\"*80)\n",
    "display(monthly_income_pivot)\n",
    "\n",
    "# Format for TypeScript/JavaScript (similar to the example provided)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPY-PASTE READY FORMAT (MONTHLY INCOME - EXAMPLE STYLE):\")\n",
    "print(\"=\"*80)\n",
    "print(\"const monthlyIncomeData = [\")\n",
    "\n",
    "for idx, row in monthly_income_pivot.iterrows():\n",
    "    # Convert date string \"2025-01\" to \"Jan 25\" format\n",
    "    month_period = pd.Period(row['date'], freq='M')\n",
    "    date_formatted = month_period.strftime('%b %y')\n",
    "    \n",
    "    # Build the object dynamically based on available categories\n",
    "    row_str = f\"  {{ date: '{date_formatted}'\"\n",
    "    \n",
    "    for col in monthly_income_pivot.columns:\n",
    "        if col != 'date':\n",
    "            value = round(row[col], 2)\n",
    "            row_str += f\", {col}: {value}\"\n",
    "    \n",
    "    row_str += \" },\"\n",
    "    print(row_str)\n",
    "\n",
    "print(\"];\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Also provide a summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total months: {len(monthly_income_pivot)}\")\n",
    "print(f\"Categories found: {[col for col in monthly_income_pivot.columns if col != 'date']}\")\n",
    "print(f\"Total income: â‚¬{monthly_income_breakdown['amount'].sum():,.2f}\")\n",
    "\n",
    "# Create breakdown by category\n",
    "if len(monthly_income_pivot.columns) > 2:  # More than just date + one category\n",
    "    print(\"\\nBreakdown by category:\")\n",
    "    for col in monthly_income_pivot.columns:\n",
    "        if col != 'date':\n",
    "            total = monthly_income_pivot[col].sum()\n",
    "            pct = (total / monthly_income_breakdown['amount'].sum()) * 100\n",
    "            print(f\"  {col}: â‚¬{total:,.2f} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5c0b7",
   "metadata": {},
   "source": [
    "## Net Cashflow Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952770fe",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Visualizes daily net cashflow as colored bars (green for positive, red for negative) with a cumulative cash position line starting from the initial balance. Shows the 30% decline in cash position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART 3: Net Cashflow (Bar) + Cumulative Cash (Line)\n",
    "\n",
    "# Get starting balance from balances.csv (first date)\n",
    "starting_balance = balances.sort_values('balance_date')['amount'].iloc[0]\n",
    "print(f\"Starting balance from balances.csv: â‚¬{starting_balance:,.2f}\")\n",
    "\n",
    "# Calculate net cashflow per day - use loaded transactions\n",
    "cashflow_data = transactions.copy()\n",
    "cashflow_data['date'] = pd.to_datetime(cashflow_data[trx_date_col])\n",
    "cashflow_data['amount_signed'] = cashflow_data.apply(\n",
    "    lambda row: row['amount'] if row['credit_or_debit'] == 'CRDT' else -row['amount'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Daily net cashflow\n",
    "daily_cashflow = cashflow_data.groupby('date')['amount_signed'].sum().reset_index()\n",
    "daily_cashflow.columns = ['date', 'net_cashflow']\n",
    "\n",
    "# Calculate cumulative cash starting from the initial balance\n",
    "daily_cashflow = daily_cashflow.sort_values('date')\n",
    "daily_cashflow['cumulative_cash'] = starting_balance + daily_cashflow['net_cashflow'].cumsum()\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig3 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add net cashflow bars\n",
    "fig3.add_trace(\n",
    "    go.Bar(\n",
    "        x=daily_cashflow['date'],\n",
    "        y=daily_cashflow['net_cashflow'],\n",
    "        name='Net Cashflow',\n",
    "        marker_color=['green' if x >= 0 else 'red' for x in daily_cashflow['net_cashflow']],\n",
    "        hovertemplate='%{x|%Y-%m-%d}<br>Net Cashflow: â‚¬%{y:,.2f}<extra></extra>'\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "# Add cumulative cash line\n",
    "fig3.add_trace(\n",
    "    go.Scatter(\n",
    "        x=daily_cashflow['date'],\n",
    "        y=daily_cashflow['cumulative_cash'],\n",
    "        name='Cumulative Cash',\n",
    "        mode='lines',\n",
    "        line=dict(color='blue', width=2),\n",
    "        hovertemplate='%{x|%Y-%m-%d}<br>Cumulative: â‚¬%{y:,.2f}<extra></extra>'\n",
    "    ),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig3.update_xaxes(title_text='Date')\n",
    "fig3.update_yaxes(title_text='Net Cashflow (EUR)', secondary_y=False)\n",
    "fig3.update_yaxes(title_text='Cumulative Cash (EUR)', secondary_y=True)\n",
    "\n",
    "fig3.update_layout(\n",
    "    title='Daily Net Cashflow & Cumulative Cash Position',\n",
    "    hovermode='x unified',\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1)\n",
    ")\n",
    "\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99732d34",
   "metadata": {},
   "source": [
    "## Forecast Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6008b91",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Parses the system_forecasts.csv file to extract daily forecasts from JSON format. Creates a flattened dataframe with one row per forecast date, method, and category combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77868465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECASTING ANALYSIS - STEP 1: Parse System Forecasts with Date Ranges\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: PARSING SYSTEM FORECASTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Parse system forecasts to create daily forecast lookup\n",
    "forecasts_daily = []\n",
    "\n",
    "for idx, row in system_forecasts.iterrows():\n",
    "    try:\n",
    "        start_date = pd.to_datetime(row['start_date'])\n",
    "        end_date = pd.to_datetime(row['end_date'])\n",
    "        method = row['forecast_method']\n",
    "        category = row['category_id']\n",
    "        \n",
    "        # Parse forecast_amounts JSON\n",
    "        forecast_data = json.loads(row['forecast_amounts'])\n",
    "        \n",
    "        for entry in forecast_data:\n",
    "            date_str = entry.get('datetime.date', '')\n",
    "            amount = entry.get('amount', 0)\n",
    "            \n",
    "            if date_str and amount is not None:\n",
    "                forecasts_daily.append({\n",
    "                    'forecast_method': method,\n",
    "                    'category': category,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date,\n",
    "                    'forecast_date': pd.to_datetime(date_str),\n",
    "                    'forecast_amount': amount\n",
    "                })\n",
    "    except (json.JSONDecodeError, TypeError, KeyError) as e:\n",
    "        print(f\"Warning: Skipping row {idx} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "forecasts_daily_df = pd.DataFrame(forecasts_daily)\n",
    "\n",
    "print(f\"\\nâœ“ Parsed {len(forecasts_daily_df):,} daily forecast entries\")\n",
    "print(f\"  Forecast methods: {forecasts_daily_df['forecast_method'].unique().tolist()}\")\n",
    "print(f\"  Categories: {forecasts_daily_df['category'].nunique()}\")\n",
    "print(f\"  Date range: {forecasts_daily_df['forecast_date'].min()} to {forecasts_daily_df['forecast_date'].max()}\")\n",
    "print(f\"\\nSample:\")\n",
    "display(forecasts_daily_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6614de4",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Matches each actual transaction to the most recent applicable forecast (by date, category, and method). This enables point-in-time forecast accuracy analysis and identifies unmatched transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18859fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECASTING ANALYSIS - STEP 2: Match Transactions to Forecasts\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: MATCHING TRANSACTIONS TO FORECASTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Categorize transactions (reuse from previous analysis)\n",
    "def categorize_transaction(row):\n",
    "    desc = str(row.get('remittence_info', '')).lower()\n",
    "    credit_or_debit = row.get('credit_or_debit', '')\n",
    "    \n",
    "    if 'resort revenue' in desc:\n",
    "        return 'cash_in_resort_revenue'\n",
    "    elif 'payroll' in desc or 'salary' in desc:\n",
    "        return 'cash_out_payroll'\n",
    "    elif 'tax' in desc and credit_or_debit == 'CRDT':\n",
    "        return 'cash_in_tax_income'\n",
    "    elif 'tax' in desc and credit_or_debit == 'DBIT':\n",
    "        return 'cash_out_tax_payments'\n",
    "    elif 'investment' in desc and 'income' in desc:\n",
    "        return 'cash_in_investments_income'\n",
    "    elif 'investment' in desc and credit_or_debit == 'DBIT':\n",
    "        return 'cash_out_investments_outflow'\n",
    "    elif 'fx' in desc or 'foreign exchange' in desc:\n",
    "        return 'cash_out_foreign_exchange_expenses'\n",
    "    elif 'insurance' in desc:\n",
    "        return 'cash_out_insurance_costs'\n",
    "    elif 'maintenance' in desc:\n",
    "        return 'cash_out_resort_maintenance_expenses'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "transactions['category'] = transactions.apply(categorize_transaction, axis=1)\n",
    "\n",
    "# Point-in-time matching: for each transaction, find the best forecast\n",
    "matched_forecasts = []\n",
    "unmatched_count = 0\n",
    "\n",
    "for idx, txn in transactions.iterrows():\n",
    "    txn_date = pd.to_datetime(txn[trx_date_col])\n",
    "    txn_category = txn['category']\n",
    "    txn_amount = txn['amount']\n",
    "    \n",
    "    # Find all forecasts that cover this date\n",
    "    candidates = forecasts_daily_df[\n",
    "        (forecasts_daily_df['forecast_date'] == txn_date) &\n",
    "        (forecasts_daily_df['category'] == txn_category) &\n",
    "        (forecasts_daily_df['start_date'] <= txn_date) &\n",
    "        (forecasts_daily_df['end_date'] >= txn_date)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(candidates) > 0:\n",
    "        # Select forecast with LATEST (most recent) start_date\n",
    "        best_idx = candidates['start_date'].idxmax()\n",
    "        best_forecast = candidates.loc[best_idx]\n",
    "        forecast_age = (txn_date - best_forecast['start_date']).days\n",
    "        \n",
    "        matched_forecasts.append({\n",
    "            'transaction_date': txn_date,\n",
    "            'category': txn_category,\n",
    "            'forecast_method': best_forecast['forecast_method'],\n",
    "            'actual_amount': txn_amount,\n",
    "            'forecast_amount': best_forecast['forecast_amount'],\n",
    "            'forecast_age': forecast_age,\n",
    "            'forecast_start_date': best_forecast['start_date']\n",
    "        })\n",
    "    else:\n",
    "        unmatched_count += 1\n",
    "\n",
    "matched_df = pd.DataFrame(matched_forecasts)\n",
    "\n",
    "print(f\"\\nâœ“ Matched {len(matched_df):,} transactions to forecasts\")\n",
    "print(f\"  Unmatched transactions: {unmatched_count}\")\n",
    "print(f\"  Categories matched: {matched_df['category'].nunique()}\")\n",
    "print(f\"  Forecast methods: {matched_df['forecast_method'].unique().tolist()}\")\n",
    "print(f\"  Average forecast age: {matched_df['forecast_age'].mean():.1f} days\")\n",
    "print(f\"\\nSample matched data:\")\n",
    "display(matched_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf09c00",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Calculates forecast accuracy metrics including MAPE (Mean Absolute Percentage Error), MAE (Mean Absolute Error), and Bias for each forecast method and category combination. Identifies which methods perform best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac497c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECASTING ANALYSIS - STEP 3: Calculate Aggregate Accuracy Metrics\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: CALCULATING ACCURACY METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate error metrics\n",
    "matched_df['abs_error'] = abs(matched_df['actual_amount'] - matched_df['forecast_amount'])\n",
    "matched_df['error'] = matched_df['forecast_amount'] - matched_df['actual_amount']\n",
    "\n",
    "# Calculate metrics by (forecast_method, category)\n",
    "def calculate_metrics(group):\n",
    "    # Filter out rows where actual is 0 for MAPE calculation\n",
    "    non_zero_actual = group[group['actual_amount'] != 0]\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': group['abs_error'].mean(),\n",
    "        'Bias': group['error'].mean(),\n",
    "        'Count': len(group),\n",
    "        'Avg_Forecast_Age': group['forecast_age'].mean()\n",
    "    }\n",
    "    \n",
    "    # Calculate MAPE only on non-zero actuals\n",
    "    if len(non_zero_actual) > 0:\n",
    "        mape_values = (non_zero_actual['abs_error'] / non_zero_actual['actual_amount'].abs()) * 100\n",
    "        metrics['MAPE'] = mape_values.mean()\n",
    "        \n",
    "        # Bias as percentage of mean actual\n",
    "        metrics['Bias_Pct'] = (group['error'].mean() / non_zero_actual['actual_amount'].abs().mean()) * 100\n",
    "    else:\n",
    "        metrics['MAPE'] = np.nan\n",
    "        metrics['Bias_Pct'] = np.nan\n",
    "    \n",
    "    return pd.Series(metrics)\n",
    "\n",
    "accuracy_df = matched_df.groupby(['forecast_method', 'category']).apply(calculate_metrics).reset_index()\n",
    "\n",
    "# Sort by MAPE (best to worst)\n",
    "accuracy_df = accuracy_df.sort_values('MAPE').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ Calculated accuracy metrics for {len(accuracy_df)} (method, category) combinations\")\n",
    "print(f\"\\nAccuracy Summary (sorted by MAPE):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "display(accuracy_df)\n",
    "\n",
    "# Best overall method (weighted by count)\n",
    "method_summary = accuracy_df.groupby('forecast_method').apply(\n",
    "    lambda g: pd.Series({\n",
    "        'MAE': (g['MAE'] * g['Count']).sum() / g['Count'].sum(),\n",
    "        'MAPE': (g['MAPE'] * g['Count']).sum() / g['Count'].sum(),\n",
    "        'Bias_Pct': (g['Bias_Pct'] * g['Count']).sum() / g['Count'].sum(),\n",
    "        'Count': g['Count'].sum(),\n",
    "        'Avg_Forecast_Age': (g['Avg_Forecast_Age'] * g['Count']).sum() / g['Count'].sum()\n",
    "    })\n",
    ").sort_values('MAPE')\n",
    "\n",
    "print(f\"\\n\\nMethod Summary (Weighted by Count):\")\n",
    "print(\"=\"*100)\n",
    "display(method_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278339e",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Reconstructs a complete daily time series (Jan-Aug 2025) for all categories and forecast methods. Uses most recent forecasts for each date and includes user forecast data. Creates foundation for unified forecast analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECASTING ANALYSIS - STEP 4: Reconstruct Daily Time Series (Jan-Aug 2025)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: RECONSTRUCTING DAILY TIME SERIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare user forecasts (exclude dismissed/cancelled)\n",
    "user_forecasts['value_date'] = pd.to_datetime(user_forecasts['value_date'], errors='coerce')\n",
    "user_forecasts_active = user_forecasts[\n",
    "    ~user_forecasts['status'].isin(['dismissed', 'cancelled'])\n",
    "].copy()\n",
    "\n",
    "# Generate complete date range\n",
    "date_range = pd.date_range('2025-01-01', '2025-08-31', freq='D')\n",
    "categories = transactions['category'].unique()\n",
    "\n",
    "daily_reconstruction = []\n",
    "\n",
    "print(f\"\\nProcessing {len(date_range)} days x {len(categories)} categories = {len(date_range) * len(categories):,} records\")\n",
    "print(\"Using MOST RECENT forecast (latest start_date) for each date...\")\n",
    "\n",
    "for date in date_range:\n",
    "    for category in categories:\n",
    "        # Get actual amount for this date and category\n",
    "        actual = transactions[\n",
    "            (pd.to_datetime(transactions[trx_date_col]) == date) & \n",
    "            (transactions['category'] == category)\n",
    "        ]['amount'].sum()\n",
    "        \n",
    "        # Make cash_out categories negative\n",
    "        if category.startswith('cash_out_'):\n",
    "            actual = -actual\n",
    "        \n",
    "        row = {\n",
    "            'date': date,\n",
    "            'category': category,\n",
    "            'actual': actual if actual != 0 else 0\n",
    "        }\n",
    "        \n",
    "        # For each forecast method, find the MOST RECENT forecast (latest start_date)\n",
    "        for method in ['ml_model', 'statistical_model', 'foundation_model', 'historical', 'static']:\n",
    "            candidates = forecasts_daily_df[\n",
    "                (forecasts_daily_df['forecast_date'] == date) &\n",
    "                (forecasts_daily_df['category'] == category) &\n",
    "                (forecasts_daily_df['forecast_method'] == method) &\n",
    "                (forecasts_daily_df['start_date'] <= date) &\n",
    "                (forecasts_daily_df['end_date'] >= date)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(candidates) > 0:\n",
    "                # Select forecast with LATEST (most recent) start_date\n",
    "                best_idx = candidates['start_date'].idxmax()\n",
    "                best_forecast = candidates.loc[best_idx]\n",
    "                row[f'forecast_{method}'] = best_forecast['forecast_amount']\n",
    "            else:\n",
    "                row[f'forecast_{method}'] = 0\n",
    "        \n",
    "        # Add user forecast if exists\n",
    "        user_forecast = user_forecasts_active[\n",
    "            (user_forecasts_active['value_date'] == date) &\n",
    "            (user_forecasts_active['category_id'] == category)\n",
    "        ]['amount'].sum()\n",
    "        row['forecast_user'] = user_forecast if user_forecast != 0 else 0\n",
    "        \n",
    "        daily_reconstruction.append(row)\n",
    "\n",
    "daily_df = pd.DataFrame(daily_reconstruction)\n",
    "\n",
    "print(f\"\\nâœ“ Reconstructed {len(daily_df):,} daily records\")\n",
    "print(f\"  Date range: {daily_df['date'].min()} to {daily_df['date'].max()}\")\n",
    "print(f\"  Categories: {daily_df['category'].nunique()}\")\n",
    "print(f\"  Days with actual transactions: {(daily_df['actual'] != 0).sum():,}\")\n",
    "print(f\"\\nSample daily reconstruction:\")\n",
    "display(daily_df[daily_df['actual'] != 0].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c10294",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Exports the daily reconstruction data to CSV format (forecast_daily_reconstruction.csv) for use by subsequent analysis steps and potential external tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECASTING ANALYSIS - STEP 5: Export Daily Reconstruction\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: EXPORTING DAILY RECONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export daily reconstruction to forecast_analysis directory\n",
    "forecast_dir = Path(\"/Users/gianniskotsas/Documents/Side Projects/palm-case-study/scripts/datasets/forecast_analysis\")\n",
    "daily_export = daily_df.copy()\n",
    "daily_export['date'] = daily_export['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "daily_path = forecast_dir / \"forecast_daily_reconstruction.csv\"\n",
    "daily_export.to_csv(daily_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Exported daily reconstruction: {daily_path}\")\n",
    "print(f\"  Records: {len(daily_export):,}\")\n",
    "print(f\"  Date range: {daily_export['date'].min()} to {daily_export['date'].max()}\")\n",
    "print(f\"  Categories: {daily_export['category'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DAILY RECONSTRUCTION EXPORTED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0118bf0",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates a unified forecast by selecting the best available system forecast (priority: ML > Statistical > Foundation > Static) and applies user overrides where finance team has provided verified/unverified forecasts. Converts all values from cents to EUR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb671bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE UNIFIED FORECAST DATASET\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING UNIFIED FORECAST DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load the forecast daily reconstruction\n",
    "forecast_dir = Path(\"/Users/gianniskotsas/Documents/Side Projects/palm-case-study/scripts/datasets/forecast_analysis\")\n",
    "recon_path = forecast_dir / \"forecast_daily_reconstruction.csv\"\n",
    "daily_recon = pd.read_csv(recon_path, parse_dates=['date'])\n",
    "\n",
    "print(f\"\\nLoaded {len(daily_recon):,} records from forecast_daily_reconstruction.csv\")\n",
    "\n",
    "# Convert forecast values from cents to EUR (divide by 100)\n",
    "forecast_columns = ['forecast_ml_model', 'forecast_statistical_model', 'forecast_foundation_model', \n",
    "                   'forecast_static', 'forecast_user']\n",
    "for col in forecast_columns:\n",
    "    daily_recon[col] = daily_recon[col] / 100\n",
    "\n",
    "print(\"âœ“ Converted forecast values from cents to EUR\")\n",
    "\n",
    "# Create unified forecast column using fallback logic: ML > Statistical > Foundation > Static\n",
    "def get_unified_forecast(row):\n",
    "    \"\"\"Select first non-zero forecast in priority order\"\"\"\n",
    "    for method in ['forecast_ml_model', 'forecast_statistical_model', 'forecast_foundation_model', 'forecast_static']:\n",
    "        value = row[method]\n",
    "        if value != 0 and pd.notna(value):\n",
    "            return value\n",
    "    return 0\n",
    "\n",
    "daily_recon['unified_forecast'] = daily_recon.apply(get_unified_forecast, axis=1)\n",
    "\n",
    "# Create unified forecast with user overrides\n",
    "# Load user forecasts and filter for verified/unverified only\n",
    "user_forecasts_override = user_forecasts[\n",
    "    user_forecasts['status'].isin(['verified', 'unverified'])\n",
    "].copy()\n",
    "user_forecasts_override['value_date'] = pd.to_datetime(user_forecasts_override['value_date'])\n",
    "\n",
    "print(f\"User forecasts to override: {len(user_forecasts_override)}\")\n",
    "\n",
    "# Start with unified forecast, then override with user forecasts\n",
    "daily_recon['unified_forecast_with_user'] = daily_recon['unified_forecast'].copy()\n",
    "\n",
    "# Apply user overrides (convert from cents to EUR)\n",
    "for idx, row in daily_recon.iterrows():\n",
    "    user_match = user_forecasts_override[\n",
    "        (user_forecasts_override['value_date'] == row['date']) &\n",
    "        (user_forecasts_override['category_id'] == row['category'])\n",
    "    ]\n",
    "    if len(user_match) > 0:\n",
    "        user_value = user_match['amount'].sum() / 100  # Convert from cents to EUR\n",
    "        if user_value != 0:\n",
    "            daily_recon.at[idx, 'unified_forecast_with_user'] = user_value\n",
    "\n",
    "# Create export dataset with selected columns\n",
    "unified_export = daily_recon[['date', 'category', 'actual', 'unified_forecast', 'unified_forecast_with_user']].copy()\n",
    "unified_export['date'] = unified_export['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Export to forecast_analysis directory\n",
    "unified_path = forecast_dir / \"unified_forecast_daily.csv\"\n",
    "unified_export.to_csv(unified_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Created unified forecast dataset: {unified_path}\")\n",
    "print(f\"  Records: {len(unified_export):,}\")\n",
    "print(f\"  Date range: {unified_export['date'].min()} to {unified_export['date'].max()}\")\n",
    "print(f\"  Categories: {unified_export['category'].nunique()}\")\n",
    "print(f\"  NOTE: All forecast values converted from cents to EUR (divided by 100)\")\n",
    "print(f\"\\nSample unified forecasts:\")\n",
    "display(unified_export[unified_export['actual'] != 0].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f8817",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Calculates monthly MAPE (Mean Absolute Percentage Error) metrics for both unified forecast types, enabling month-by-month accuracy comparison and identification of which months are most/least predictable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30af455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE MONTHLY MAPE METRICS FOR UNIFIED FORECASTS\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CALCULATING MONTHLY MAPE METRICS (JAN 1 - AUG 31, 2025)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"NOTE: Using EUR values (forecasts already converted from cents)\\n\")\n",
    "\n",
    "# Filter data for the period Jan 1 - Aug 31, 2025\n",
    "daily_recon_filtered = daily_recon[\n",
    "    (daily_recon['date'] >= '2025-01-01') & \n",
    "    (daily_recon['date'] <= '2025-08-31')\n",
    "].copy()\n",
    "\n",
    "# Add month column\n",
    "daily_recon_filtered['month'] = daily_recon_filtered['date'].dt.to_period('M')\n",
    "\n",
    "print(f\"\\nFiltered to {len(daily_recon_filtered):,} records for Jan 1 - Aug 31, 2025\")\n",
    "\n",
    "# Calculate MAPE for both unified forecast types by month and category\n",
    "mape_results = []\n",
    "\n",
    "for forecast_type in ['unified_forecast', 'unified_forecast_with_user']:\n",
    "    print(f\"\\n--- {forecast_type.replace('_', ' ').title()} ---\")\n",
    "    \n",
    "    # Filter out rows where actual = 0 to avoid division by zero\n",
    "    data_with_actuals = daily_recon_filtered[daily_recon_filtered['actual'] != 0].copy()\n",
    "    \n",
    "    # Calculate absolute percentage error\n",
    "    data_with_actuals['abs_pct_error'] = (\n",
    "        abs(data_with_actuals['actual'] - data_with_actuals[forecast_type]) / \n",
    "        abs(data_with_actuals['actual'])\n",
    "    ) * 100\n",
    "    \n",
    "    # Per-month, per-category MAPE\n",
    "    monthly_category_mape = data_with_actuals.groupby(['month', 'category']).agg({\n",
    "        'abs_pct_error': 'mean',\n",
    "        'actual': 'count'\n",
    "    }).reset_index()\n",
    "    monthly_category_mape.columns = ['month', 'category', 'mape', 'count']\n",
    "    monthly_category_mape['forecast_type'] = forecast_type\n",
    "    \n",
    "    # Total MAPE per month (treating all data points equally)\n",
    "    monthly_total_mape = data_with_actuals.groupby('month')['abs_pct_error'].mean().reset_index()\n",
    "    monthly_total_mape.columns = ['month', 'total_mape']\n",
    "    \n",
    "    # Merge with category data\n",
    "    monthly_category_mape = monthly_category_mape.merge(monthly_total_mape, on='month')\n",
    "    \n",
    "    print(f\"  Categories analyzed: {monthly_category_mape['category'].nunique()}\")\n",
    "    print(f\"  Months analyzed: {monthly_category_mape['month'].nunique()}\")\n",
    "    print(f\"  Data points: {len(data_with_actuals):,}\")\n",
    "    \n",
    "    # Add to results\n",
    "    for _, row in monthly_category_mape.iterrows():\n",
    "        mape_results.append({\n",
    "            'month': str(row['month']),\n",
    "            'category': row['category'],\n",
    "            'forecast_type': forecast_type,\n",
    "            'mape': row['mape'],\n",
    "            'count': row['count'],\n",
    "            'total_mape': row['total_mape']\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "mape_df = pd.DataFrame(mape_results)\n",
    "\n",
    "# Export to CSV\n",
    "mape_path = forecast_dir / \"mape_metrics.csv\"\n",
    "mape_df.to_csv(mape_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Exported monthly MAPE metrics: {mape_path}\")\n",
    "print(f\"  Records: {len(mape_df)}\")\n",
    "print(f\"\\nMonthly MAPE Summary by Forecast Type:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display summary by month\n",
    "monthly_summary = mape_df.groupby(['month', 'forecast_type']).agg({\n",
    "    'mape': 'mean',\n",
    "    'total_mape': 'first',\n",
    "    'count': 'sum'\n",
    "}).reset_index()\n",
    "monthly_summary.columns = ['Month', 'Forecast Type', 'Avg Category MAPE', 'Total MAPE', 'Total Data Points']\n",
    "display(monthly_summary)\n",
    "\n",
    "print(f\"\\nTop 5 Best Categories (Unified Forecast):\")\n",
    "best_unified = mape_df[mape_df['forecast_type'] == 'unified_forecast'].nsmallest(5, 'mape')[['month', 'category', 'mape', 'count']]\n",
    "display(best_unified)\n",
    "\n",
    "print(f\"\\nTop 5 Worst Categories (Unified Forecast):\")\n",
    "worst_unified = mape_df[mape_df['forecast_type'] == 'unified_forecast'].nlargest(5, 'mape')[['month', 'category', 'mape', 'count']]\n",
    "display(worst_unified)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b883b8",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates a detailed line chart showing daily resort revenue (actual in black) compared to all forecast methods. Helps visualize which methods track actual revenue most closely and where gaps occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART: RESORT REVENUE - ACTUAL VS ALL FORECASTS\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING RESORT REVENUE CHART\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter for resort revenue\n",
    "revenue_data = daily_recon[daily_recon['category'] == 'cash_in_resort_revenue'].copy()\n",
    "revenue_data = revenue_data.sort_values('date')\n",
    "\n",
    "print(f\"\\nRevenue data points: {len(revenue_data)}\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=revenue_data['date'],\n",
    "    y=revenue_data['actual'],\n",
    "    mode='lines',\n",
    "    name='Actual',\n",
    "    line=dict(color='black', width=3),\n",
    "    hovertemplate='%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add forecast model lines\n",
    "forecast_lines = [\n",
    "    ('forecast_ml_model', 'ML Model', '#1f77b4'),\n",
    "    ('forecast_statistical_model', 'Statistical Model', '#ff7f0e'),\n",
    "    ('forecast_foundation_model', 'Foundation Model', '#2ca02c'),\n",
    "    ('forecast_static', 'Static', '#d62728'),\n",
    "    ('forecast_user', 'User Forecasts', '#9467bd')\n",
    "]\n",
    "\n",
    "for col, name, color in forecast_lines:\n",
    "    # Replace 0 with NaN to create gaps in the line\n",
    "    y_data = revenue_data[col].replace(0, np.nan)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=revenue_data['date'],\n",
    "        y=y_data,\n",
    "        mode='lines',\n",
    "        name=name,\n",
    "        line=dict(color=color, width=2, dash='dash' if 'user' in col.lower() else 'solid'),\n",
    "        hovertemplate='%{y:,.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Resort Revenue: Actual vs All Forecasts (EUR)',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Amount (EUR)'),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ“ Resort revenue chart displayed\")\n",
    "print(\"  NOTE: Forecast values displayed in EUR (converted from cents)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6a9b9",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates a detailed line chart showing daily payroll expenses (actual in black) compared to all forecast methods. Reveals forecasting challenges for this significant expense category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd11054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART: PAYROLL EXPENSES - ACTUAL VS ALL FORECASTS\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING PAYROLL EXPENSES CHART\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter for payroll expenses\n",
    "payroll_data = daily_recon[daily_recon['category'] == 'cash_out_payroll'].copy()\n",
    "payroll_data = payroll_data.sort_values('date')\n",
    "\n",
    "print(f\"\\nPayroll data points: {len(payroll_data)}\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=payroll_data['date'],\n",
    "    y=payroll_data['actual'],\n",
    "    mode='lines',\n",
    "    name='Actual',\n",
    "    line=dict(color='black', width=3),\n",
    "    hovertemplate='%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add forecast model lines\n",
    "forecast_lines = [\n",
    "    ('forecast_ml_model', 'ML Model', '#1f77b4'),\n",
    "    ('forecast_statistical_model', 'Statistical Model', '#ff7f0e'),\n",
    "    ('forecast_foundation_model', 'Foundation Model', '#2ca02c'),\n",
    "    ('forecast_static', 'Static', '#d62728'),\n",
    "    ('forecast_user', 'User Forecasts', '#9467bd')\n",
    "]\n",
    "\n",
    "for col, name, color in forecast_lines:\n",
    "    # Replace 0 with NaN to create gaps in the line\n",
    "    y_data = payroll_data[col].replace(0, np.nan)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=payroll_data['date'],\n",
    "        y=y_data,\n",
    "        mode='lines',\n",
    "        name=name,\n",
    "        line=dict(color=color, width=2, dash='dash' if 'user' in col.lower() else 'solid'),\n",
    "        hovertemplate='%{y:,.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Payroll Expenses: Actual vs All Forecasts (EUR)',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Amount (EUR)'),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ“ Payroll expenses chart displayed\")\n",
    "print(\"  NOTE: Forecast values displayed in EUR (converted from cents)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94a520",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Generates monthly aggregated chart comparing actual resort revenue to unified forecast and unified forecast with user overrides. Shows overall monthly accuracy at a higher level than daily comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef204654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART: RESORT REVENUE - ACTUAL VS UNIFIED FORECASTS (MONTHLY)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING MONTHLY RESORT REVENUE UNIFIED FORECAST CHART\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter for resort revenue\n",
    "revenue_unified = daily_recon[daily_recon['category'] == 'cash_in_resort_revenue'].copy()\n",
    "revenue_unified = revenue_unified.sort_values('date')\n",
    "\n",
    "# Add month column\n",
    "revenue_unified['month'] = revenue_unified['date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate to monthly\n",
    "monthly_revenue = revenue_unified.groupby('month').agg({\n",
    "    'actual': 'sum',\n",
    "    'unified_forecast': 'sum',\n",
    "    'unified_forecast_with_user': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Convert month to string for display\n",
    "monthly_revenue['month_display'] = monthly_revenue['month'].apply(lambda x: x.strftime('%b %Y'))\n",
    "\n",
    "print(f\"\\nMonthly revenue data points: {len(monthly_revenue)}\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_revenue['month_display'],\n",
    "    y=monthly_revenue['actual'],\n",
    "    mode='lines+markers',\n",
    "    name='Actual',\n",
    "    line=dict(color='black', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add unified forecast line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_revenue['month_display'],\n",
    "    y=monthly_revenue['unified_forecast'],\n",
    "    mode='lines+markers',\n",
    "    name='Unified Forecast',\n",
    "    line=dict(color='#2196F3', width=2, dash='dash'),\n",
    "    marker=dict(size=6),\n",
    "    hovertemplate='â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add unified forecast with user overrides line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_revenue['month_display'],\n",
    "    y=monthly_revenue['unified_forecast_with_user'],\n",
    "    mode='lines+markers',\n",
    "    name='Unified Forecast + User',\n",
    "    line=dict(color='#FF9800', width=2, dash='dot'),\n",
    "    marker=dict(size=6),\n",
    "    hovertemplate='â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Monthly Resort Revenue: Actual vs Unified Forecasts (EUR)',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='Amount (EUR)'),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ“ Monthly resort revenue unified forecast chart displayed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fefeb9",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Generates monthly aggregated chart comparing actual payroll expenses to unified forecast and unified forecast with user overrides. Enables assessment of monthly payroll predictability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e558d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART: PAYROLL EXPENSES - ACTUAL VS UNIFIED FORECASTS (MONTHLY)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING MONTHLY PAYROLL EXPENSES UNIFIED FORECAST CHART\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter for payroll expenses\n",
    "payroll_unified = daily_recon[daily_recon['category'] == 'cash_out_payroll'].copy()\n",
    "payroll_unified = payroll_unified.sort_values('date')\n",
    "\n",
    "# Add month column\n",
    "payroll_unified['month'] = payroll_unified['date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate to monthly\n",
    "monthly_payroll = payroll_unified.groupby('month').agg({\n",
    "    'actual': 'sum',\n",
    "    'unified_forecast': 'sum',\n",
    "    'unified_forecast_with_user': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Convert month to string for display\n",
    "monthly_payroll['month_display'] = monthly_payroll['month'].apply(lambda x: x.strftime('%b %Y'))\n",
    "\n",
    "print(f\"\\nMonthly payroll data points: {len(monthly_payroll)}\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_payroll['month_display'],\n",
    "    y=monthly_payroll['actual'],\n",
    "    mode='lines+markers',\n",
    "    name='Actual',\n",
    "    line=dict(color='black', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add unified forecast line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_payroll['month_display'],\n",
    "    y=monthly_payroll['unified_forecast'],\n",
    "    mode='lines+markers',\n",
    "    name='Unified Forecast',\n",
    "    line=dict(color='#2196F3', width=2, dash='dash'),\n",
    "    marker=dict(size=6),\n",
    "    hovertemplate='â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add unified forecast with user overrides line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_payroll['month_display'],\n",
    "    y=monthly_payroll['unified_forecast_with_user'],\n",
    "    mode='lines+markers',\n",
    "    name='Unified Forecast + User',\n",
    "    line=dict(color='#FF9800', width=2, dash='dot'),\n",
    "    marker=dict(size=6),\n",
    "    hovertemplate='â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Monthly Payroll Expenses: Actual vs Unified Forecasts (EUR)',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='Amount (EUR)'),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ“ Monthly payroll expenses unified forecast chart displayed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d0f36",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates comparison chart and statistical summary showing monthly MAPE for unified forecast vs unified forecast with user overrides. Quantifies whether user interventions improve or degrade forecast accuracy. Includes detailed monthly breakdowns and overall statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11cb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART: MONTHLY MAPE COMPARISON - UNIFIED FORECAST vs UNIFIED FORECAST WITH USER\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING MONTHLY MAPE COMPARISON CHART\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load the monthly MAPE data\n",
    "mape_df = pd.read_csv(forecast_dir / \"mape_metrics.csv\")\n",
    "\n",
    "# Calculate average MAPE per month for each forecast type\n",
    "monthly_mape_summary = mape_df.groupby(['month', 'forecast_type'])['mape'].mean().reset_index()\n",
    "\n",
    "# Pivot to get forecast types as columns\n",
    "monthly_mape_pivot = monthly_mape_summary.pivot(index='month', columns='forecast_type', values='mape').reset_index()\n",
    "\n",
    "# Convert month strings to proper format for display\n",
    "monthly_mape_pivot['month_display'] = monthly_mape_pivot['month'].apply(\n",
    "    lambda x: pd.Period(x).strftime('%b %Y')\n",
    ")\n",
    "\n",
    "print(f\"\\nMonthly MAPE data points: {len(monthly_mape_pivot)}\")\n",
    "print(f\"Months: {monthly_mape_pivot['month_display'].tolist()}\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add line for unified_forecast\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_mape_pivot['month_display'],\n",
    "    y=monthly_mape_pivot['unified_forecast'],\n",
    "    mode='lines+markers',\n",
    "    name='Unified Forecast',\n",
    "    line=dict(color='#2196F3', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='%{x}<br>Unified Forecast MAPE: %{y:.2f}%<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add line for unified_forecast_with_user\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=monthly_mape_pivot['month_display'],\n",
    "    y=monthly_mape_pivot['unified_forecast_with_user'],\n",
    "    mode='lines+markers',\n",
    "    name='Unified Forecast + User',\n",
    "    line=dict(color='#FF9800', width=3),\n",
    "    marker=dict(size=8),\n",
    "    hovertemplate='%{x}<br>Unified Forecast + User MAPE: %{y:.2f}%<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Monthly MAPE Comparison: Unified Forecast vs Unified Forecast + User',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='MAPE (%)'),\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nâœ“ Monthly MAPE comparison chart displayed\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MONTHLY MAPE SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate improvement from user overrides\n",
    "monthly_mape_pivot['improvement'] = monthly_mape_pivot['unified_forecast'] - monthly_mape_pivot['unified_forecast_with_user']\n",
    "monthly_mape_pivot['improvement_pct'] = (monthly_mape_pivot['improvement'] / monthly_mape_pivot['unified_forecast']) * 100\n",
    "\n",
    "print(\"\\nMonthly MAPE Values:\")\n",
    "for idx, row in monthly_mape_pivot.iterrows():\n",
    "    print(f\"{row['month_display']}:\")\n",
    "    print(f\"  Unified Forecast: {row['unified_forecast']:.2f}%\")\n",
    "    print(f\"  Unified + User:   {row['unified_forecast_with_user']:.2f}%\")\n",
    "    print(f\"  Improvement:      {row['improvement']:.2f}% ({row['improvement_pct']:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# Overall statistics\n",
    "avg_unified = monthly_mape_pivot['unified_forecast'].mean()\n",
    "avg_unified_user = monthly_mape_pivot['unified_forecast_with_user'].mean()\n",
    "overall_improvement = avg_unified - avg_unified_user\n",
    "overall_improvement_pct = (overall_improvement / avg_unified) * 100\n",
    "\n",
    "print(f\"Overall Average MAPE:\")\n",
    "print(f\"  Unified Forecast: {avg_unified:.2f}%\")\n",
    "print(f\"  Unified + User:   {avg_unified_user:.2f}%\")\n",
    "print(f\"  Average Improvement: {overall_improvement:.2f}% ({overall_improvement_pct:.1f}%)\")\n",
    "\n",
    "# Best and worst months\n",
    "best_month_idx = monthly_mape_pivot['unified_forecast_with_user'].idxmin()\n",
    "worst_month_idx = monthly_mape_pivot['unified_forecast_with_user'].idxmax()\n",
    "\n",
    "print(f\"\\nBest Month (Lowest MAPE): {monthly_mape_pivot.loc[best_month_idx, 'month_display']} ({monthly_mape_pivot.loc[best_month_idx, 'unified_forecast_with_user']:.2f}%)\")\n",
    "print(f\"Worst Month (Highest MAPE): {monthly_mape_pivot.loc[worst_month_idx, 'month_display']} ({monthly_mape_pivot.loc[worst_month_idx, 'unified_forecast_with_user']:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d7fc6",
   "metadata": {},
   "source": [
    "## Daily Forecast Comparison by Category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48765928",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Loads the previously generated forecast reconstruction data to prepare for category-by-category visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forecast reconstruction data\n",
    "forecast_recon_path = Path(\"/Users/gianniskotsas/Documents/Side Projects/palm-case-study/scripts/datasets/forecast_analysis/forecast_daily_reconstruction.csv\")\n",
    "forecast_df = pd.read_csv(forecast_recon_path, parse_dates=['date'])\n",
    "\n",
    "# Get unique categories\n",
    "categories = forecast_df['category'].unique()\n",
    "print(f\"Categories found: {len(categories)}\")\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237786b",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Generates individual line charts for each transaction category showing actual values vs all forecast methods. Provides detailed visual analysis of forecast performance per category with color-coded forecast lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate line charts for each category\n",
    "forecast_columns = [\n",
    "    'forecast_ml_model',\n",
    "    'forecast_statistical_model', \n",
    "    'forecast_foundation_model',\n",
    "    'forecast_historical',\n",
    "    'forecast_static',\n",
    "    'forecast_user'\n",
    "]\n",
    "\n",
    "# Define colors for each forecast type\n",
    "forecast_colors = {\n",
    "    'forecast_ml_model': '#FF6B6B',\n",
    "    'forecast_statistical_model': '#4ECDC4',\n",
    "    'forecast_foundation_model': '#45B7D1',\n",
    "    'forecast_historical': '#FFA07A',\n",
    "    'forecast_static': '#98D8C8',\n",
    "    'forecast_user': '#C7CEEA'\n",
    "}\n",
    "\n",
    "for category in categories:\n",
    "    # Filter data for this category\n",
    "    category_df = forecast_df[forecast_df['category'] == category].sort_values('date')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add actual line (solid)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=category_df['date'],\n",
    "        y=category_df['actual'],\n",
    "        mode='lines',\n",
    "        name='Actual',\n",
    "        line=dict(color='#2C3E50', width=2, dash='solid'),\n",
    "        hovertemplate='<b>Actual</b><br>Date: %{x}<br>Value: â‚¬%{y:,.2f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Add forecast lines (dotted)\n",
    "    for forecast_col in forecast_columns:\n",
    "        forecast_name = forecast_col.replace('forecast_', '').replace('_', ' ').title()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=category_df['date'],\n",
    "            y=category_df[forecast_col]/100,\n",
    "            mode='lines',\n",
    "            name=forecast_name,\n",
    "            line=dict(color=forecast_colors[forecast_col], width=1.5, dash='dot'),\n",
    "            hovertemplate=f'<b>{forecast_name}</b><br>Date: %{{x}}<br>Value: â‚¬%{{y:,.2f}}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    category_title = category.replace('_', ' ').title()\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Daily Forecast vs Actual: {category_title}',\n",
    "            font=dict(size=16, color='#2C3E50')\n",
    "        ),\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Amount (â‚¬)',\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            orientation='v',\n",
    "            yanchor='top',\n",
    "            y=1,\n",
    "            xanchor='left',\n",
    "            x=1.02\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=500,\n",
    "        margin=dict(r=200)\n",
    "    )\n",
    "    \n",
    "    # Show figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb3d0c",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481557e8",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Comprehensive data quality analysis that generates JSON files for four key areas: 1) Balance Reconciliation (transaction vs snapshot comparison), 2) User Forecast Quality (status breakdown and verification rates), 3) System Forecast Coverage (gaps by method and category), 4) Transaction Data Quality (completeness and anomalies). All outputs feed the data quality dashboard section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA QUALITY ANALYSIS - Generate JSON files for frontend\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data_quality folder\n",
    "output_dir = base_dir.parent / 'data_quality'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if required DataFrames exist\n",
    "if 'forecasts_daily_df' not in locals():\n",
    "    print(\"âš ï¸  Warning: forecasts_daily_df not found. Please run the forecasting analysis cells first.\")\n",
    "    print(\"   This analysis requires the parsed system forecasts from earlier cells.\")\n",
    "else:\n",
    "    print(f\"âœ“ Using forecasts_daily_df with {len(forecasts_daily_df):,} daily forecast records\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BALANCE RECONCILIATION ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n1. BALANCE RECONCILIATION ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Prepare balances data\n",
    "bal = balances.copy()\n",
    "bal['date'] = pd.to_datetime(bal['balance_date'])\n",
    "bal['month'] = bal['date'].dt.to_period('M')\n",
    "\n",
    "# Get first and last balance per month from snapshots\n",
    "monthly_bal = bal.groupby('month').agg({\n",
    "    'amount': ['first', 'last'],\n",
    "    'date': ['min', 'max']\n",
    "}).reset_index()\n",
    "monthly_bal.columns = ['month', 'start_balance_snapshot', 'end_balance_snapshot', 'start_date', 'end_date']\n",
    "\n",
    "# Calculate expected balances based on transactions\n",
    "tx_calc = transactions.copy()\n",
    "tx_calc['date'] = pd.to_datetime(tx_calc[trx_date_col])\n",
    "tx_calc['month'] = tx_calc['date'].dt.to_period('M')\n",
    "tx_calc['amount_signed'] = tx_calc.apply(\n",
    "    lambda row: row['amount'] if row['credit_or_debit'] == 'CRDT' else -row['amount'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate monthly net change from transactions\n",
    "monthly_tx_change = tx_calc.groupby('month')['amount_signed'].sum().reset_index()\n",
    "monthly_tx_change.columns = ['month', 'tx_net_change']\n",
    "\n",
    "# Merge with balance data\n",
    "balance_comparison = monthly_bal.merge(monthly_tx_change, on='month', how='left')\n",
    "balance_comparison['tx_net_change'] = balance_comparison['tx_net_change'].fillna(0)\n",
    "\n",
    "# Calculate expected end balance and deviation\n",
    "balance_comparison['expected_end_balance'] = balance_comparison['start_balance_snapshot'] + balance_comparison['tx_net_change']\n",
    "balance_comparison['deviation'] = balance_comparison['end_balance_snapshot'] - balance_comparison['expected_end_balance']\n",
    "balance_comparison['deviation_pct'] = (balance_comparison['deviation'] / balance_comparison['end_balance_snapshot'] * 100).round(2)\n",
    "balance_comparison['month_str'] = balance_comparison['month'].astype(str)\n",
    "\n",
    "# Find largest discrepancy\n",
    "largest_discrepancy_idx = balance_comparison['deviation'].abs().idxmax()\n",
    "largest_discrepancy = balance_comparison.loc[largest_discrepancy_idx]\n",
    "\n",
    "balance_reconciliation_data = {\n",
    "    \"monthly_deviations\": balance_comparison[[\n",
    "        'month_str', 'start_balance_snapshot', 'end_balance_snapshot', \n",
    "        'expected_end_balance', 'deviation', 'deviation_pct'\n",
    "    ]].to_dict('records'),\n",
    "    \"total_deviation\": float(balance_comparison['deviation'].sum()),\n",
    "    \"avg_deviation\": float(balance_comparison['deviation'].mean()),\n",
    "    \"largest_discrepancy\": {\n",
    "        \"month\": str(largest_discrepancy['month_str']),\n",
    "        \"deviation\": float(largest_discrepancy['deviation']),\n",
    "        \"deviation_pct\": float(largest_discrepancy['deviation_pct']),\n",
    "        \"expected\": float(largest_discrepancy['expected_end_balance']),\n",
    "        \"actual\": float(largest_discrepancy['end_balance_snapshot'])\n",
    "    },\n",
    "    \"months_analyzed\": len(balance_comparison)\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_dir / 'data_quality_balance_reconciliation.json', 'w') as f:\n",
    "    json.dump(balance_reconciliation_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Total Deviation: â‚¬{balance_reconciliation_data['total_deviation']:,.2f}\")\n",
    "print(f\"âœ“ Average Deviation: â‚¬{balance_reconciliation_data['avg_deviation']:,.2f}\")\n",
    "print(f\"âœ“ Largest Discrepancy: {largest_discrepancy['month_str']} (â‚¬{largest_discrepancy['deviation']:,.2f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. USER FORECAST QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n2. USER FORECAST QUALITY ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Status breakdown\n",
    "status_counts = user_forecasts['status'].value_counts().to_dict()\n",
    "total_user_forecasts = len(user_forecasts)\n",
    "status_percentages = {k: round(v / total_user_forecasts * 100, 2) for k, v in status_counts.items()}\n",
    "\n",
    "# Forecasts with missing/zero amounts\n",
    "missing_amounts = user_forecasts[user_forecasts['amount'].isna()].shape[0]\n",
    "zero_amounts = user_forecasts[user_forecasts['amount'] == 0].shape[0]\n",
    "\n",
    "# By forecast source\n",
    "source_counts = user_forecasts['forecast_source'].value_counts().to_dict()\n",
    "\n",
    "# By category (top 5)\n",
    "category_counts = user_forecasts['category_id'].value_counts().head(10).to_dict()\n",
    "\n",
    "user_forecast_data = {\n",
    "    \"total_forecasts\": total_user_forecasts,\n",
    "    \"status_breakdown\": {\n",
    "        \"counts\": status_counts,\n",
    "        \"percentages\": status_percentages\n",
    "    },\n",
    "    \"missing_data\": {\n",
    "        \"missing_amounts\": int(missing_amounts),\n",
    "        \"zero_amounts\": int(zero_amounts),\n",
    "        \"total_problematic\": int(missing_amounts + zero_amounts)\n",
    "    },\n",
    "    \"forecast_sources\": source_counts,\n",
    "    \"top_categories\": category_counts,\n",
    "    \"quality_score\": round((status_counts.get('verified', 0) / total_user_forecasts) * 100, 1)\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_dir / 'data_quality_user_forecasts.json', 'w') as f:\n",
    "    json.dump(user_forecast_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Total User Forecasts: {total_user_forecasts:,}\")\n",
    "print(f\"âœ“ Verified: {status_counts.get('verified', 0)} ({status_percentages.get('verified', 0)}%)\")\n",
    "print(f\"âœ“ Dismissed: {status_counts.get('dismissed', 0)} ({status_percentages.get('dismissed', 0)}%)\")\n",
    "print(f\"âœ“ Cancelled: {status_counts.get('cancelled', 0)} ({status_percentages.get('cancelled', 0)}%)\")\n",
    "print(f\"âœ“ Quality Score: {user_forecast_data['quality_score']}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. SYSTEM FORECAST COVERAGE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n3. SYSTEM FORECAST COVERAGE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if forecasts_daily_df exists\n",
    "if 'forecasts_daily_df' not in locals():\n",
    "    print(\"âš ï¸  Skipping system forecast coverage - forecasts_daily_df not found\")\n",
    "    print(\"   Please run the forecasting analysis cells first\")\n",
    "    \n",
    "    # Create empty placeholder data\n",
    "    system_forecast_coverage_data = {\n",
    "        \"coverage_by_method\": {},\n",
    "        \"overall_coverage\": {},\n",
    "        \"gaps_by_category\": {},\n",
    "        \"date_range\": {\"start\": \"2025-01-01\", \"end\": \"2025-08-31\", \"total_days\": 243},\n",
    "        \"categories_analyzed\": 0,\n",
    "        \"methods_analyzed\": 0\n",
    "    }\n",
    "else:\n",
    "    # Generate complete date range\n",
    "    date_range = pd.date_range('2025-01-01', '2025-08-31', freq='D')\n",
    "    total_days = len(date_range)\n",
    "\n",
    "    # Get unique categories from transactions\n",
    "    categories = transactions['category'].unique()\n",
    "    forecast_methods = ['ml_model', 'statistical_model', 'foundation_model', 'static']\n",
    "\n",
    "    coverage_by_method = {}\n",
    "    gaps_by_category = {}\n",
    "\n",
    "    # Use the parsed forecasts_daily_df from earlier analysis\n",
    "    for method in forecast_methods:\n",
    "        method_forecasts = forecasts_daily_df[forecasts_daily_df['forecast_method'] == method].copy()\n",
    "        \n",
    "        # For each category, calculate coverage\n",
    "        category_coverage = {}\n",
    "        for category in categories:\n",
    "            category_forecasts = method_forecasts[method_forecasts['category'] == category]\n",
    "            \n",
    "            # Count how many days have forecasts (within our date range)\n",
    "            forecast_dates = category_forecasts['forecast_date']\n",
    "            forecast_dates_in_range = forecast_dates[(forecast_dates >= date_range[0]) & (forecast_dates <= date_range[-1])]\n",
    "            coverage_days = len(forecast_dates_in_range.unique())\n",
    "            coverage_pct = (coverage_days / total_days) * 100\n",
    "            \n",
    "            category_coverage[category] = {\n",
    "                \"covered_days\": int(coverage_days),\n",
    "                \"total_days\": int(total_days),\n",
    "                \"coverage_pct\": round(coverage_pct, 2),\n",
    "                \"missing_days\": int(total_days - coverage_days)\n",
    "            }\n",
    "            \n",
    "            # Track gaps\n",
    "            if category not in gaps_by_category:\n",
    "                gaps_by_category[category] = {}\n",
    "            gaps_by_category[category][method] = int(total_days - coverage_days)\n",
    "        \n",
    "        coverage_by_method[method] = category_coverage\n",
    "\n",
    "    # Calculate overall coverage per method\n",
    "    overall_coverage = {}\n",
    "    for method in forecast_methods:\n",
    "        total_possible = len(categories) * total_days\n",
    "        total_covered = sum([coverage_by_method[method][cat]['covered_days'] for cat in categories])\n",
    "        overall_coverage[method] = {\n",
    "            \"coverage_pct\": round((total_covered / total_possible) * 100, 2),\n",
    "            \"covered\": int(total_covered),\n",
    "            \"total\": int(total_possible)\n",
    "        }\n",
    "\n",
    "    system_forecast_coverage_data = {\n",
    "        \"coverage_by_method\": coverage_by_method,\n",
    "        \"overall_coverage\": overall_coverage,\n",
    "        \"gaps_by_category\": gaps_by_category,\n",
    "        \"date_range\": {\n",
    "            \"start\": \"2025-01-01\",\n",
    "            \"end\": \"2025-08-31\",\n",
    "            \"total_days\": int(total_days)\n",
    "        },\n",
    "        \"categories_analyzed\": len(categories),\n",
    "        \"methods_analyzed\": len(forecast_methods)\n",
    "    }\n",
    "\n",
    "    print(f\"âœ“ Categories Analyzed: {len(categories)}\")\n",
    "    print(f\"âœ“ Date Range: 2025-01-01 to 2025-08-31 ({total_days} days)\")\n",
    "    for method, cov in overall_coverage.items():\n",
    "        print(f\"  - {method}: {cov['coverage_pct']}% coverage\")\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_dir / 'data_quality_system_forecast_gaps.json', 'w') as f:\n",
    "    json.dump(system_forecast_coverage_data, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TRANSACTION DATA QUALITY\n",
    "# ============================================================================\n",
    "print(\"\\n4. TRANSACTION DATA QUALITY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Date range\n",
    "tx_date_min = transactions[trx_date_col].min()\n",
    "tx_date_max = transactions[trx_date_col].max()\n",
    "total_transactions = len(transactions)\n",
    "\n",
    "# Count by category\n",
    "transactions_by_category = transactions['category'].value_counts().to_dict()\n",
    "\n",
    "# Potential duplicates (same date, amount, and bank_reference)\n",
    "if 'bank_reference' in transactions.columns:\n",
    "    dup_cols = [trx_date_col, 'amount', 'bank_reference']\n",
    "    potential_duplicates = transactions.duplicated(subset=dup_cols, keep=False).sum()\n",
    "else:\n",
    "    dup_cols = [trx_date_col, 'amount']\n",
    "    potential_duplicates = transactions.duplicated(subset=dup_cols, keep=False).sum()\n",
    "\n",
    "# Missing critical fields\n",
    "missing_dates = transactions[trx_date_col].isna().sum()\n",
    "missing_amounts = transactions['amount'].isna().sum()\n",
    "missing_categories = transactions['category'].isna().sum() if 'category' in transactions.columns else 0\n",
    "\n",
    "# Credit/Debit distribution\n",
    "credit_debit_counts = transactions['credit_or_debit'].value_counts().to_dict()\n",
    "\n",
    "transaction_quality_data = {\n",
    "    \"total_transactions\": int(total_transactions),\n",
    "    \"date_range\": {\n",
    "        \"min_date\": str(tx_date_min.date()),\n",
    "        \"max_date\": str(tx_date_max.date()),\n",
    "        \"days_span\": int((tx_date_max - tx_date_min).days)\n",
    "    },\n",
    "    \"transactions_by_category\": transactions_by_category,\n",
    "    \"credit_debit_distribution\": credit_debit_counts,\n",
    "    \"data_quality_flags\": {\n",
    "        \"potential_duplicates\": int(potential_duplicates),\n",
    "        \"missing_dates\": int(missing_dates),\n",
    "        \"missing_amounts\": int(missing_amounts),\n",
    "        \"missing_categories\": int(missing_categories),\n",
    "        \"total_issues\": int(potential_duplicates + missing_dates + missing_amounts + missing_categories)\n",
    "    },\n",
    "    \"quality_score\": round(((total_transactions - potential_duplicates - missing_dates - missing_amounts) / total_transactions) * 100, 1)\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_dir / 'data_quality_transactions.json', 'w') as f:\n",
    "    json.dump(transaction_quality_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Total Transactions: {total_transactions:,}\")\n",
    "print(f\"âœ“ Date Range: {tx_date_min.date()} to {tx_date_max.date()}\")\n",
    "print(f\"âœ“ Potential Duplicates: {potential_duplicates}\")\n",
    "print(f\"âœ“ Missing Data: {missing_dates + missing_amounts + missing_categories} records\")\n",
    "print(f\"âœ“ Quality Score: {transaction_quality_data['quality_score']}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL DATA QUALITY FILES GENERATED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFiles saved to: {output_dir.relative_to(Path.cwd().parent)}\")\n",
    "print(\"  - data_quality_balance_reconciliation.json\")\n",
    "print(\"  - data_quality_user_forecasts.json\")\n",
    "print(\"  - data_quality_system_forecast_gaps.json\")\n",
    "print(\"  - data_quality_transactions.json\")\n",
    "print(\"\\nðŸ’¡ Next steps:\")\n",
    "print(\"  1. Files are ready for the Next.js frontend\")\n",
    "print(\"  2. API routes will read from: scripts/datasets/data_quality/\")\n",
    "print(\"  3. Refresh your browser to see the data quality section populated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4dd13",
   "metadata": {},
   "source": [
    "### Balance Reconciliation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e886d569",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Creates a bar chart showing monthly balance deviations between actual balance snapshots and expected balances calculated from transactions. Identifies months with the largest discrepancies indicating potential data issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHART 4: Monthly Balance Deviations\n",
    "\n",
    "# Confirm which date column is being used\n",
    "print(f\"Using date column: {trx_date_col}\")\n",
    "\n",
    "# Prepare balances data - use loaded balances\n",
    "bal = balances.copy()\n",
    "bal['date'] = pd.to_datetime(bal['balance_date'])\n",
    "bal['month'] = bal['date'].dt.to_period('M')\n",
    "\n",
    "# Get first and last balance per month from snapshots\n",
    "monthly_bal = bal.groupby('month').agg({\n",
    "    'amount': ['first', 'last'],\n",
    "    'date': ['min', 'max']\n",
    "}).reset_index()\n",
    "monthly_bal.columns = ['month', 'start_balance_snapshot', 'end_balance_snapshot', 'start_date', 'end_date']\n",
    "\n",
    "# Calculate expected balances based on transactions - use loaded transactions\n",
    "tx_calc = transactions.copy()\n",
    "tx_calc['date'] = pd.to_datetime(tx_calc[trx_date_col])\n",
    "tx_calc['month'] = tx_calc['date'].dt.to_period('M')\n",
    "tx_calc['amount_signed'] = tx_calc.apply(\n",
    "    lambda row: row['amount'] if row['credit_or_debit'] == 'CRDT' else -row['amount'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate monthly net change from transactions\n",
    "monthly_tx_change = tx_calc.groupby('month')['amount_signed'].sum().reset_index()\n",
    "monthly_tx_change.columns = ['month', 'tx_net_change']\n",
    "\n",
    "# Merge with balance data\n",
    "balance_comparison = monthly_bal.merge(monthly_tx_change, on='month', how='left')\n",
    "balance_comparison['tx_net_change'] = balance_comparison['tx_net_change'].fillna(0)\n",
    "\n",
    "# Calculate expected end balance\n",
    "balance_comparison['expected_end_balance'] = balance_comparison['start_balance_snapshot'] + balance_comparison['tx_net_change']\n",
    "\n",
    "# Calculate deviation\n",
    "balance_comparison['deviation'] = balance_comparison['end_balance_snapshot'] - balance_comparison['expected_end_balance']\n",
    "balance_comparison['month_str'] = balance_comparison['month'].astype(str)\n",
    "\n",
    "# Create chart - only showing deviation\n",
    "fig4 = go.Figure()\n",
    "\n",
    "# Add deviation bars only\n",
    "fig4.add_trace(go.Bar(\n",
    "    x=balance_comparison['month_str'],\n",
    "    y=balance_comparison['deviation'],\n",
    "    name='Deviation',\n",
    "    marker_color=['red' if x < 0 else 'green' for x in balance_comparison['deviation']],\n",
    "    hovertemplate='%{x}<br>Deviation: â‚¬%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add a zero reference line\n",
    "fig4.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Zero Deviation\")\n",
    "\n",
    "fig4.update_layout(\n",
    "    title='Monthly Balance Deviations: Actual vs Expected (from Transactions)',\n",
    "    xaxis=dict(title='Month'),\n",
    "    yaxis=dict(title='Deviation (EUR)', zeroline=True),\n",
    "    hovermode='x unified',\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig4.show()\n",
    "\n",
    "# Display deviation summary\n",
    "print(\"\\nBalance Deviation Summary:\")\n",
    "print(balance_comparison[['month_str', 'start_balance_snapshot', 'tx_net_change', \n",
    "                           'expected_end_balance', 'end_balance_snapshot', 'deviation']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa4997",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Performs comprehensive data exploration including: column analysis, missing value assessment, date range validation, duplicate detection, sign consistency checks, and overall data quality scoring. Provides foundation for understanding data reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a80cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: DATA EXPLORATION & QUALITY ASSESSMENT\n",
    "from typing import Optional\n",
    "\n",
    "if transactions is None:\n",
    "    display(\"Transactions dataset not found. Please ensure 'transactions.csv' exists under datasets/raw.\")\n",
    "else:\n",
    "    # Helpers to detect amount column and coerce numerics\n",
    "    AMOUNT_CANDIDATES = [\n",
    "        'amount', 'booking_amount', 'transaction_amount', 'value', 'amt'\n",
    "    ]\n",
    "\n",
    "    def pick_amount_column(df: pd.DataFrame) -> Optional[str]:\n",
    "        for c in AMOUNT_CANDIDATES:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        # guess: first numeric column named like amount\n",
    "        for c in df.columns:\n",
    "            if 'amount' in c.lower() or 'value' in c.lower():\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    def coerce_amount(series: pd.Series) -> pd.Series:\n",
    "        # If string with comma decimal, replace and convert\n",
    "        if series.dtype == 'object' or pd.api.types.is_string_dtype(series):\n",
    "            return pd.to_numeric(series.str.replace(',', '.', regex=False).str.replace(' ', ''), errors='coerce')\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "\n",
    "    amount_col = pick_amount_column(transactions)\n",
    "    if amount_col is None:\n",
    "        # Try to infer from credits/debits\n",
    "        amount_col = 'amount'\n",
    "        transactions[amount_col] = pd.to_numeric(0)\n",
    "\n",
    "    # Coerce date and amount columns\n",
    "    if trx_date_col is not None:\n",
    "        transactions[trx_date_col] = pd.to_datetime(transactions[trx_date_col], errors='coerce')\n",
    "    transactions[amount_col] = coerce_amount(transactions[amount_col])\n",
    "\n",
    "    # Preview head/tail\n",
    "    display(transactions.head(5))\n",
    "    display(transactions.tail(5))\n",
    "\n",
    "    # Column names and dtypes\n",
    "    display(pd.DataFrame({'dtype': transactions.dtypes.astype(str)}))\n",
    "\n",
    "    # Missingness\n",
    "    missing_counts = transactions.isna().sum().sort_values(ascending=False)\n",
    "    missing_pct = (transactions.isna().mean() * 100).round(2)\n",
    "    dq_missing = pd.concat([missing_counts.rename('missing_count'), missing_pct.rename('missing_%')], axis=1)\n",
    "    display(dq_missing)\n",
    "\n",
    "    # Date range\n",
    "    date_min = transactions[trx_date_col].min() if trx_date_col else None\n",
    "    date_max = transactions[trx_date_col].max() if trx_date_col else None\n",
    "    print({'date_min': date_min, 'date_max': date_max})\n",
    "\n",
    "    # Unique transaction categories/types if present\n",
    "    category_like_cols = [c for c in transactions.columns if 'category' in c.lower() or 'type' in c.lower()]\n",
    "    unique_info = {}\n",
    "    for c in category_like_cols:\n",
    "        nun = transactions[c].nunique(dropna=True)\n",
    "        sample_vals = transactions[c].dropna().astype(str).value_counts().head(10)\n",
    "        unique_info[c] = {'nunique': nun, 'top_values': sample_vals.to_dict()}\n",
    "    display(unique_info)\n",
    "\n",
    "    # Basic amount statistics\n",
    "    amount_stats = transactions[amount_col].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).to_frame('amount_stats')\n",
    "    display(amount_stats)\n",
    "\n",
    "    # Obvious data quality issues\n",
    "    # 1) Duplicates\n",
    "    dup_cols = [trx_date_col, amount_col, 'bank_reference'] if 'bank_reference' in transactions.columns else [trx_date_col, amount_col]\n",
    "    potential_dups = transactions.duplicated(subset=[c for c in dup_cols if c is not None], keep=False).sum()\n",
    "\n",
    "    # 2) Non-parsable dates or amounts\n",
    "    non_parsable_dates = transactions[trx_date_col].isna().sum() if trx_date_col else None\n",
    "    non_parsable_amounts = transactions[amount_col].isna().sum()\n",
    "\n",
    "    # 3) Sign consistency vs credit_or_debit\n",
    "    sign_inconsistencies = None\n",
    "    if 'credit_or_debit' in transactions.columns:\n",
    "        cod = transactions['credit_or_debit'].astype(str).str.upper()\n",
    "        # infer sign from amount if signed; if all positive, we cannot check reliably\n",
    "        if transactions[amount_col].notna().any():\n",
    "            sign = np.sign(transactions[amount_col].fillna(0))\n",
    "            sign_inconsistencies = ((cod == 'CRDT') & (sign < 0) | (cod.str.startswith(('DB', 'DR'))) & (sign > 0)).sum()\n",
    "\n",
    "    dq_summary = pd.DataFrame([\n",
    "        {'metric': 'rows', 'value': len(transactions)},\n",
    "        {'metric': 'columns', 'value': transactions.shape[1]},\n",
    "        {'metric': 'date_min', 'value': date_min},\n",
    "        {'metric': 'date_max', 'value': date_max},\n",
    "        {'metric': 'potential_duplicates', 'value': int(potential_dups)},\n",
    "        {'metric': 'non_parsable_dates', 'value': None if non_parsable_dates is None else int(non_parsable_dates)},\n",
    "        {'metric': 'non_parsable_amounts', 'value': int(non_parsable_amounts)},\n",
    "        {'metric': 'sign_inconsistencies', 'value': None if sign_inconsistencies is None else int(sign_inconsistencies)},\n",
    "    ])\n",
    "    display(dq_summary)\n",
    "\n",
    "    # Balances presence and date coverage\n",
    "    if balances is not None:\n",
    "        bmin = balances['balance_date'].min() if 'balance_date' in balances.columns else None\n",
    "        bmax = balances['balance_date'].max() if 'balance_date' in balances.columns else None\n",
    "        print({'balances_date_min': bmin, 'balances_date_max': bmax, 'balances_rows': len(balances)})\n",
    "\n",
    "    # Forecast datasets presence\n",
    "    print({'system_forecasts_present': system_forecasts is not None, 'user_forecasts_present': user_forecasts is not None})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85675e14",
   "metadata": {},
   "source": [
    "### Data Quality: Category Direction & Outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4e9d1",
   "metadata": {},
   "source": [
    "**Code Cell Description:**\n",
    "\n",
    "Comprehensive outlier analysis using two methods: 1) IQR (Interquartile Range) method and 2) Z-Score method. Identifies transactions that deviate significantly from normal patterns by category, validates transaction sign consistency (cash_in vs cash_out), and generates visualizations including box plots, scatter plots, and distribution histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4068306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL: DATA QUALITY & OUTLIER ANALYSIS\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY ANALYSIS: CATEGORY DIRECTION VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First, check if category column exists, otherwise create it\n",
    "if 'category' not in transactions.columns:\n",
    "    print(\"\\nâš ï¸  'category' column not found - creating it now...\")\n",
    "    \n",
    "    def categorize_transaction(row):\n",
    "        desc = str(row.get('remittence_info', '')).lower()\n",
    "        credit_or_debit = row.get('credit_or_debit', '')\n",
    "        \n",
    "        if 'resort revenue' in desc:\n",
    "            return 'cash_in_resort_revenue'\n",
    "        elif 'payroll' in desc or 'salary' in desc:\n",
    "            return 'cash_out_payroll'\n",
    "        elif 'tax payment' in desc:\n",
    "            return 'cash_out_tax_payments'\n",
    "        elif 'insurance' in desc:\n",
    "            return 'cash_out_insurance_costs'\n",
    "        elif 'maintenance' in desc:\n",
    "            return 'cash_out_resort_maintenance_expenses'\n",
    "        elif 'investment' in desc and credit_or_debit == 'DBIT':\n",
    "            return 'cash_out_investments_outflow'\n",
    "        elif 'investment' in desc and credit_or_debit == 'CRDT':\n",
    "            return 'cash_in_investments_income'\n",
    "        elif 'foreign exchange' in desc or 'fx fee' in desc:\n",
    "            return 'cash_out_foreign_exchange_expenses'\n",
    "        elif 'tax' in desc and credit_or_debit == 'CRDT':\n",
    "            return 'cash_in_tax_income'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    transactions['category'] = transactions.apply(categorize_transaction, axis=1)\n",
    "    print(\"âœ“ Category column created\")\n",
    "\n",
    "# Get unique categories\n",
    "all_categories_unique = transactions['category'].unique()\n",
    "cash_in_categories = [cat for cat in all_categories_unique if cat.startswith('cash_in_')]\n",
    "cash_out_categories = [cat for cat in all_categories_unique if cat.startswith('cash_out_')]\n",
    "\n",
    "print(f\"\\nFound {len(all_categories_unique)} unique transaction categories:\")\n",
    "print(f\"  Cash IN categories: {len(cash_in_categories)}\")\n",
    "for cat in cash_in_categories:\n",
    "    count = (transactions['category'] == cat).sum()\n",
    "    print(f\"    - {cat}: {count} transactions\")\n",
    "print(f\"\\n  Cash OUT categories: {len(cash_out_categories)}\")\n",
    "for cat in cash_out_categories:\n",
    "    count = (transactions['category'] == cat).sum()\n",
    "    print(f\"    - {cat}: {count} transactions\")\n",
    "\n",
    "# Check for negative values in cash_in categories\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. CASH_IN CATEGORIES - Checking for negative amounts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cash_in_issues = []\n",
    "for category in cash_in_categories:\n",
    "    cat_data = transactions[transactions['category'] == category]\n",
    "    negative_count = (cat_data['amount'] < 0).sum()\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        negative_sum = cat_data[cat_data['amount'] < 0]['amount'].sum()\n",
    "        negative_samples = cat_data[cat_data['amount'] < 0][['value_date', 'amount', 'remittence_info']].head()\n",
    "        \n",
    "        cash_in_issues.append({\n",
    "            'category': category,\n",
    "            'negative_count': negative_count,\n",
    "            'negative_sum': negative_sum,\n",
    "            'total_in_category': len(cat_data),\n",
    "            'percentage': (negative_count / len(cat_data)) * 100\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâš ï¸  {category}:\")\n",
    "        print(f\"    - Found {negative_count} negative values ({(negative_count / len(cat_data)) * 100:.2f}% of this category)\")\n",
    "        print(f\"    - Total negative amount: â‚¬{negative_sum:,.2f}\")\n",
    "        print(f\"    - Sample transactions:\")\n",
    "        for idx, row in negative_samples.iterrows():\n",
    "            print(f\"      {row['value_date']}: â‚¬{row['amount']:,.2f} - {row['remittence_info'][:50]}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“  {category}: All {len(cat_data)} transactions have positive amounts\")\n",
    "\n",
    "# Check for positive values in cash_out categories\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. CASH_OUT CATEGORIES - Checking for positive amounts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cash_out_issues = []\n",
    "for category in cash_out_categories:\n",
    "    cat_data = transactions[transactions['category'] == category]\n",
    "    positive_count = (cat_data['amount'] > 0).sum()\n",
    "    \n",
    "    if positive_count > 0:\n",
    "        positive_sum = cat_data[cat_data['amount'] > 0]['amount'].sum()\n",
    "        positive_samples = cat_data[cat_data['amount'] > 0][['value_date', 'amount', 'remittence_info']].head()\n",
    "        \n",
    "        cash_out_issues.append({\n",
    "            'category': category,\n",
    "            'positive_count': positive_count,\n",
    "            'positive_sum': positive_sum,\n",
    "            'total_in_category': len(cat_data),\n",
    "            'percentage': (positive_count / len(cat_data)) * 100\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâš ï¸  {category}:\")\n",
    "        print(f\"    - Found {positive_count} positive values ({(positive_count / len(cat_data)) * 100:.2f}% of this category)\")\n",
    "        print(f\"    - Total positive amount: â‚¬{positive_sum:,.2f}\")\n",
    "        print(f\"    - Sample transactions:\")\n",
    "        for idx, row in positive_samples.iterrows():\n",
    "            print(f\"      {row['value_date']}: â‚¬{row['amount']:,.2f} - {row['remittence_info'][:50]}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“  {category}: All {len(cat_data)} transactions have negative amounts\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORY DIRECTION VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if cash_in_issues:\n",
    "    print(f\"\\nâš ï¸  Found {len(cash_in_issues)} cash_in categories with negative values:\")\n",
    "    for issue in cash_in_issues:\n",
    "        print(f\"   - {issue['category']}: {issue['negative_count']} issues ({issue['percentage']:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All cash_in categories have correct (positive) values\")\n",
    "\n",
    "if cash_out_issues:\n",
    "    print(f\"\\nâš ï¸  Found {len(cash_out_issues)} cash_out categories with positive values:\")\n",
    "    for issue in cash_out_issues:\n",
    "        print(f\"   - {issue['category']}: {issue['positive_count']} issues ({issue['percentage']:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nâœ“ All cash_out categories have correct (negative) values\")\n",
    "\n",
    "# ============================================================================\n",
    "# OUTLIER ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER ANALYSIS: ALL TRANSACTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter out 'other' category for outlier analysis\n",
    "analysis_categories = [cat for cat in all_categories_unique if cat != 'other']\n",
    "\n",
    "print(f\"\\nAnalyzing {len(analysis_categories)} categories across {len(transactions)} transactions\")\n",
    "print(f\"Categories: {', '.join(analysis_categories)}\")\n",
    "\n",
    "# Create a dataframe with all transaction amounts and their categories\n",
    "outlier_data = []\n",
    "for idx, row in transactions.iterrows():\n",
    "    category = row['category']\n",
    "    amount = row['amount']\n",
    "    \n",
    "    if category in analysis_categories and pd.notna(amount) and amount != 0:\n",
    "        outlier_data.append({\n",
    "            'index': idx,\n",
    "            'category': category,\n",
    "            'amount': abs(amount),  # Use absolute value for outlier detection\n",
    "            'original_value': amount,\n",
    "            'date': row[trx_date_col] if pd.notna(row[trx_date_col]) else None,\n",
    "            'description': row.get('remittence_info', '')[:50]\n",
    "        })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_data)\n",
    "\n",
    "if len(outlier_df) > 0:\n",
    "    print(f\"\\nTotal non-zero transactions analyzed: {len(outlier_df):,}\")\n",
    "    print(f\"Amount range: â‚¬{outlier_df['amount'].min():,.2f} to â‚¬{outlier_df['amount'].max():,.2f}\")\n",
    "    print(f\"Mean: â‚¬{outlier_df['amount'].mean():,.2f}, Median: â‚¬{outlier_df['amount'].median():,.2f}\")\n",
    "    \n",
    "    # Method 1: IQR Method (Interquartile Range)\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"METHOD 1: IQR (Interquartile Range) Method\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    Q1 = outlier_df['amount'].quantile(0.25)\n",
    "    Q3 = outlier_df['amount'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    iqr_outliers = outlier_df[(outlier_df['amount'] < lower_bound) | (outlier_df['amount'] > upper_bound)].copy()\n",
    "    iqr_outliers = iqr_outliers.sort_values('amount', ascending=False)\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Q1 (25th percentile): â‚¬{Q1:,.2f}\")\n",
    "    print(f\"  Q3 (75th percentile): â‚¬{Q3:,.2f}\")\n",
    "    print(f\"  IQR: â‚¬{IQR:,.2f}\")\n",
    "    print(f\"  Lower bound: â‚¬{lower_bound:,.2f}\")\n",
    "    print(f\"  Upper bound: â‚¬{upper_bound:,.2f}\")\n",
    "    print(f\"\\nðŸŽ¯ Outliers found: {len(iqr_outliers)} ({(len(iqr_outliers)/len(outlier_df)*100):.2f}%)\")\n",
    "    \n",
    "    if len(iqr_outliers) > 0:\n",
    "        print(f\"\\nTop 25 IQR Outliers (by absolute amount):\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Date':<12} {'Category':<40} {'Amount':>15} {'Description':<30}\")\n",
    "        print(\"-\" * 100)\n",
    "        for i, row in iqr_outliers.head(25).iterrows():\n",
    "            date_str = row['date'].strftime('%Y-%m-%d') if pd.notna(row['date']) else 'N/A'\n",
    "            cat_short = row['category'].replace('cash_in_', 'IN:').replace('cash_out_', 'OUT:')\n",
    "            print(f\"{date_str:<12} {cat_short:<40} â‚¬{row['original_value']:>13,.2f} {row['description'][:28]:<30}\")\n",
    "    \n",
    "    # Method 2: Z-Score Method\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"METHOD 2: Z-Score Method (|z| > 3)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    mean_amount = outlier_df['amount'].mean()\n",
    "    std_amount = outlier_df['amount'].std()\n",
    "    \n",
    "    outlier_df['z_score'] = (outlier_df['amount'] - mean_amount) / std_amount\n",
    "    z_outliers = outlier_df[abs(outlier_df['z_score']) > 3].copy()\n",
    "    z_outliers = z_outliers.sort_values('z_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Mean: â‚¬{mean_amount:,.2f}\")\n",
    "    print(f\"  Std Dev: â‚¬{std_amount:,.2f}\")\n",
    "    print(f\"  Threshold: |z| > 3 (99.7% confidence)\")\n",
    "    print(f\"\\nðŸŽ¯ Outliers found: {len(z_outliers)} ({(len(z_outliers)/len(outlier_df)*100):.2f}%)\")\n",
    "    \n",
    "    if len(z_outliers) > 0:\n",
    "        print(f\"\\nTop 25 Z-Score Outliers:\")\n",
    "        print(\"-\" * 110)\n",
    "        print(f\"{'Date':<12} {'Category':<40} {'Amount':>15} {'Z-Score':>8} {'Description':<25}\")\n",
    "        print(\"-\" * 110)\n",
    "        for i, row in z_outliers.head(25).iterrows():\n",
    "            date_str = row['date'].strftime('%Y-%m-%d') if pd.notna(row['date']) else 'N/A'\n",
    "            cat_short = row['category'].replace('cash_in_', 'IN:').replace('cash_out_', 'OUT:')\n",
    "            print(f\"{date_str:<12} {cat_short:<40} â‚¬{row['original_value']:>13,.2f} {row['z_score']:>8.2f} {row['description'][:23]:<25}\")\n",
    "    \n",
    "    # Category-wise outlier analysis\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CATEGORY-WISE OUTLIER SUMMARY (IQR Method)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    category_outlier_summary = []\n",
    "    for category in analysis_categories:\n",
    "        cat_data = outlier_df[outlier_df['category'] == category].copy()\n",
    "        if len(cat_data) > 0:\n",
    "            cat_Q1 = cat_data['amount'].quantile(0.25)\n",
    "            cat_Q3 = cat_data['amount'].quantile(0.75)\n",
    "            cat_IQR = cat_Q3 - cat_Q1\n",
    "            cat_lower = cat_Q1 - 1.5 * cat_IQR\n",
    "            cat_upper = cat_Q3 + 1.5 * cat_IQR\n",
    "            \n",
    "            cat_outliers = cat_data[(cat_data['amount'] < cat_lower) | (cat_data['amount'] > cat_upper)]\n",
    "            \n",
    "            category_outlier_summary.append({\n",
    "                'category': category,\n",
    "                'total_txns': len(cat_data),\n",
    "                'outliers': len(cat_outliers),\n",
    "                'outlier_pct': (len(cat_outliers) / len(cat_data)) * 100 if len(cat_data) > 0 else 0,\n",
    "                'median': cat_data['amount'].median(),\n",
    "                'mean': cat_data['amount'].mean(),\n",
    "                'max': cat_data['amount'].max(),\n",
    "                'upper_bound': cat_upper\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(category_outlier_summary).sort_values('outliers', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'Category':<40} {'Total':>8} {'Outliers':>10} {'%':>7} {'Median':>12} {'Mean':>12} {'Max':>12}\")\n",
    "    print(\"-\" * 115)\n",
    "    for _, row in summary_df.iterrows():\n",
    "        cat_short = row['category'].replace('cash_in_', 'IN:').replace('cash_out_', 'OUT:')\n",
    "        print(f\"{cat_short:<40} {row['total_txns']:>8,} {row['outliers']:>10,} {row['outlier_pct']:>6.1f}% â‚¬{row['median']:>10,.0f} â‚¬{row['mean']:>10,.0f} â‚¬{row['max']:>10,.0f}\")\n",
    "    \n",
    "    # Visualization: Box plot of all categories\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING OUTLIER VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create box plot for each category\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for category in sorted(analysis_categories):\n",
    "        cat_data = outlier_df[outlier_df['category'] == category]['amount']\n",
    "        cat_short = category.replace('cash_in_', 'IN:').replace('cash_out_', 'OUT:')\n",
    "        fig.add_trace(go.Box(\n",
    "            y=cat_data,\n",
    "            name=cat_short,\n",
    "            boxmean='sd',\n",
    "            marker_color='lightblue' if category.startswith('cash_in_') else 'lightcoral'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Transaction Amount Distribution by Category (Absolute Values, Log Scale)',\n",
    "        yaxis_title='Amount (â‚¬)',\n",
    "        xaxis_title='Category',\n",
    "        height=600,\n",
    "        width=1400,\n",
    "        showlegend=True,\n",
    "        yaxis_type='log',\n",
    "        xaxis={'tickangle': -45}\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create scatter plot of outliers over time\n",
    "    if len(z_outliers) > 0:\n",
    "        print(\"\\nGenerating outliers over time visualization...\")\n",
    "        \n",
    "        outliers_with_dates = z_outliers[z_outliers['date'].notna()].copy()\n",
    "        \n",
    "        if len(outliers_with_dates) > 0:\n",
    "            fig2 = px.scatter(\n",
    "                outliers_with_dates,\n",
    "                x='date',\n",
    "                y='amount',\n",
    "                color='category',\n",
    "                size='z_score',\n",
    "                hover_data={'original_value': ':,.2f', 'z_score': ':.2f', 'description': True, 'amount': ':,.2f'},\n",
    "                title=f'Significant Outliers Over Time (Z-Score > 3) - {len(outliers_with_dates)} transactions',\n",
    "                labels={'amount': 'Amount (â‚¬, absolute)', 'date': 'Date'},\n",
    "                height=600,\n",
    "                width=1400\n",
    "            )\n",
    "            \n",
    "            fig2.update_layout(\n",
    "                yaxis_type='log',\n",
    "                xaxis_title='Date',\n",
    "                yaxis_title='Amount (â‚¬, log scale)'\n",
    "            )\n",
    "            \n",
    "            fig2.show()\n",
    "            \n",
    "    # Distribution histogram\n",
    "    print(\"\\nGenerating amount distribution histogram...\")\n",
    "    \n",
    "    fig3 = px.histogram(\n",
    "        outlier_df,\n",
    "        x='amount',\n",
    "        color='category',\n",
    "        nbins=50,\n",
    "        title='Transaction Amount Distribution (All Categories)',\n",
    "        labels={'amount': 'Amount (â‚¬, absolute)', 'count': 'Number of Transactions'},\n",
    "        height=500,\n",
    "        width=1400,\n",
    "        log_y=True\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(\n",
    "        xaxis_type='log',\n",
    "        xaxis_title='Amount (â‚¬, log scale)',\n",
    "        yaxis_title='Count (log scale)'\n",
    "    )\n",
    "    \n",
    "    fig3.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No transaction data available for outlier analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69323fb1",
   "metadata": {},
   "source": [
    "## Summary & Instructions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (palm-scripts)",
   "language": "python",
   "name": "palm-scripts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
